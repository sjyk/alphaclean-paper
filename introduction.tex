\section{Introduction}\label{intro}\sloppy
Data cleaning is widely recognized as a major challenge in almost all forms of data analytics~\cite{nytimes}, and an analyst may spend upwards of 80\% of her time to clean and prepare the data. 
\emph{Cell inconsistencies} are an important class of data errors where record values are missing, incorrect, contain inconsistent references to the same entities, or contain artifacts from the extraction process.  
Improperly handled errors can affect the performance and accuracy of downstream applications such as reports, visualizations, and machine learning models.
In response, the research community has developed increasingly sophisticated methods for detecting and automatically repairing errors in large datasets~\cite{dc, rekatsinas2017holoclean, DBLP:journals/pvldb/KrishnanWWFG16, DBLP:conf/sigmod/ChuIKW16, mudgal2018deep, doan2018toward}.
The burden on the analyst is gradually shifting away from the design of hand-written data cleaning scripts, to building and tuning complex pipelines of automated data cleaning libraries.

Even so, designing reliable and accurate cleaning pipelines can be very challenging~\cite{krishnan2016hilda}.
Despite all of these components being under the umbrella of ``data cleaning'', there is no unified way to specify a global objective or reason about interactions between components; each has its own abstractions, parameter space, and guarantees (or lack thereof).
For example, constraint-resolution systems use integrity constraints to specify objectives~\cite{rekatsinas2017holoclean,DBLP:conf/sigmod/ChuIKW16}, while numerical outlier detection techniques use thresholds~\cite{bailis2016macrobase}.
In query optimization jargon, there is no objective definition of a \emph{plan space} (set of ways to compose and tune a pipeline of data cleaning operations) and \emph{cost model} (a way of evaluating the value of a pipeline).

This paper proposes a framework, called \sys, that provides a simple unified abstraction of cleaning transformations and facilitates optimization over a plan space of cleaning transformation sequences.
An important insight is that repairs for cell inconsistency errors can be expressed as the composition of conditional assignment statements, i.e., \texttt{set(pred, attr, target\_val)} sets \texttt{attr} to a target value if the record satisfies \texttt{pred}.  By modeling data cleaning operators and systems as functions that return sets of conditional assignment statements, we are able to model a uniform data cleaning plan-space as the set of all compositions (sequences and parallel evaluation) of all conditional assignment statements that can be generated by a set of data cleaning operators.  Clearly, it is impractical to materialize all possible conditional assignments and search through the plan space, thus a key technical challenge is an efficient search procedure that does not require materializing candidate condition assignments apriori.

% Rather than treating each pipeline component as a black-box transformation of an entire table, \sys extracts the constituent replacements. Given a library of data cleaning methods, each suggests potential replacements policies which are aggregated into a central set. This defines a well-posed plan-space, namely, the set of all compositions of candidate functions. This plan space captures method reordering, method exclusion, and applying a method to a subset of records.

Searching a plan space requires a well-defined cost model.  However, there is no fixed cost model for data cleaning---cleanliness is in the eyes of the beholder. Data quality issues are often identified as part of developing an application and interacting with the data~\cite{krishnan2016hilda}, and data scientists have an evolving definition of what quality measures are important.  The use of a cost function and cleaning plan search space is not unique to \sys.  Existing data cleaning systems~\cite{} support predefined cost functions (e.g., number of constraint violations~\cite{}, number of outliers~\cite{}, etc) and develop efficient search algorithms of a reduced plan space by leveraging properties of the cost functions.  For instance, \ewu{XXX}.  

The primary difference is that, instead of a fixed cost function, \sys supports an extensible cost model so that the user can mix and match existing or custom cost functions.  This lets the user iteratively refine the cost function to reflect the nature of the data errors throughout the cleaning process.  The cost model is a weighted sum over incrementally updatable SQL aggregates over the input database or application output\ewu{CHECK WITH SANJAY}.  o aid the user,  \sys provides a library that translates common classes of data errors such as denial constraint violations, numerical outliers, and other statistical inconsistencies into cost models.
% This model is general enough that it can describe a large variety of data quality measures, and be efficient to incrementally evaluate when possible, e.g., with fast incremental view maintenance techniques~\cite{DBLP:journals/vldb/KochAKNNLS14,krishnan2015svc}.   

The risk of decoupling the cost model from the search algorithm is that the search performance will be slow.  The plan space is enormous (our experiments encounter branching factors in the millions), and the cost model is not known a priori.  Thus, \textbf{our primary hypothesis is that data cleaning problems rarely encounter pathological cases, and thus a general search algorithm combined with recent search optimization heuristics from AI can efficiently search the space}.

We treat plan generation as a greedy tree-search problem.  Each path is a candidate data cleaning pipeline\footnote{We use plans and pipelines interchangeably, because cleaning plans are sequences of conditional assignments.} whose quality is evaluated by executing the pipeline over the input dataset and then evaluating the cost model.  Each path expansion step requires selecting a data cleaning operator to parameterize, evaluating the operator to select one of its condition assignment outputs.  Thus it is important to avoid fully evaluating a path's quality and expanding unpromising paths.    To do so, \sys dynamically learns a model that predicts the expected quality of a given cleaning pipeline.  This model thus avoids the need to execute the pipeline and quality function in order to evaluate a given path, and can be tuned to have a low false positive rate when pruning candidate paths.  A further benefit is that tree search can be easily parallelized across candidate paths, and can exploit properties of the cost model to parallelize across partitions of the dataset.  We use periodic synchronization to update the prediction model across parallel searches and merge transformations that touch disjoint sets of data.  

\noindent To summarize our contributions:

\begin{itemize}[leftmargin=*, topsep=0mm, itemsep=0mm]
  \item A framework, \sys, which allows for extensible generation of a data cleaning plan space from existing cleaning methods and an flexible API for defining custom quality measures to search the space.
  \item A progressive search algorithm that quickly generates acceptable cleaning pipelines, adaptively prunes the search space, and a suite of pragmatic optimizations including multi-node parallelization and data sharing to reduce network communication bottlenecks that reduce runtimes by \ewu{XXX$\times$}.   
  \item A systematic study of the benefits and limitations of \sys in terms of data cleaning accuracy (precision, recall), and the runtime.  We show that \sys can solve incremental refinements of the data quality measure $yyy\times$ faster than from scratch, and that \ewu{SOME OTHER FINDING}
\end{itemize}



