\section{Introduction}\label{intro}\sloppy
Data cleaning is widely recognized as a major challenge in almost all forms of data analytics~\cite{nytimes}. 
Analysts report spending upwards of 80\% of analysis time during data cleaning and preparation.
Improperly handled errors can affect the performance and accuracy of downstream applications such as reports, visualizations, and machine learning models.
In response, the research community has developed a number of sophisticated data cleaning libraries for detecting and repairing errors in large datasets~\cite{dc, rekatsinas2017holoclean, DBLP:journals/pvldb/KrishnanWWFG16, DBLP:conf/sigmod/ChuIKW16, mudgal2018deep, doan2018toward}.
As a consequence, the burden on the analyst is gradually shifting away from the design of hand-written data cleaning scripts to building and tuning pipelines of automated data cleaning libraries~\cite{krishnan2016hilda}.

Systems to automatically optimize these pipelines and their parameters are desirable.
An initial architecture is to directly apply recent hyperparameter tuning approaches for machine learning pipelines and neural network model search~\cite{li2017hyperband, sparks2017keystoneml, baylor2017tfx, golovin2017google, liaw2018tune}.
We can treat an entire data cleaning pipeline as a parametrized black box exposing tunable parameters such as confidence thresholds and editing penalties. We can quantify the success or failure of a parameter setting with a final data quality objective function (e.g., number of tuples violating integrity constraints or cross-referencing with master data).
The tuning system will then search over possible parameter settings to optimize the objective function.


Hyperparameter tuning systems are fundamentally ill-suited for data cleaning applications.
They only assume query access to the final objective value and neglect any structure and opportunities for shared computation in the pipeline.
For example, even if the objective function was based solely on integrity constraint violations, a black-box tuning system would not recognize that integrity constraints can be incrementally checked without full re-computation~\cite{fan2014incremental}.
Similarly, these systems would not recognize opportunities for re-ordering the application of libraries and excluding irrelevant libraries.

We present a new framework called \sys whose main insight is that a common intermediate representation for repairs can facilitate more efficient data cleaning pipeline optimization.
Many popular data cleaning libraries actually ``speak the same language'', where all of their repairs can be cast as cell-replacements operations~\cite{rekatsinas2017holoclean,DBLP:conf/sigmod/ChuIKW16, DBLP:journals/pvldb/KrishnanWWFG16}.
In \sys, rather than treating the entire pipeline as a single parameterized black-box, the system assess the fine-grained repairs from each parameter setting and re-orders, excludes, and merges accordingly.

Users interface their existing data cleaning libraries to \sys with minimal code that exposes an input interface to set parameters and an output interface to collect proposed edits to a table.
Libraries can be as narrow or as general as the user desires.
For example, they can be domain specific string matching functions or entire data cleaning systems such as HoloClean~\cite{rekatsinas2017holoclean}.
Each of these libraries suggests \emph{candidate repairs} to a central pool.
Users define a data quality function (the objective) with SQL aggregation queries (allowing for UDAF's) over the input table.  This subsumes popular quality measures such as integrity constraint violations~\cite{ilyas2015trends} and numerical outlier detection~\cite{bailis2017macrobase}, and can readily express application-specific quality measures such as machine learning training accuracy or goodness-of-fit to a model. 
Separate threads search through the pool of candidates to decide a sequence of repairs to construct (a cleaning pipeline) that optimizes this quality function.
\sys works in an ``anytime'' fashion where results are progressively returned to the user.

The search algorithm is implemented as a greedy tree-search that sequences the repairs~\cite{russell2016artificial}.
The space of possible repair sequences is enormous (our experiments encounter branching factors in the millions).
Thus, it is important to avoid fully evaluating a path's quality and expanding unpromising paths.    
\sys dynamically learns a model to avoid executing the pipeline and quality function in order to evaluate a given path, and can be tuned to have a low false positive rate when pruning candidate paths.  
Furthermore, the tree search can be easily parallelized across both candidate paths, as well as across partitions of the dataset based on properties of the quality function.
We use periodic synchronization to update the prediction model across parallel searches and merge transformations that repair disjoint sets of tuples. 

\sys contributes a new architecture to data cleaning optimization.
 This flexibility in composing different quality functions can help users across different domains evaluate different notions of quality within a single system. 
 The intermediate representation and generate-then-search paradigm allows for intelligent composition of multiple systems. 
Even in cases where a existing cleaning system is specifically designed for the errors in the dataset (e.g., integrity constraints), \sys can combine other cleaning operators to further improve the repairs.  Our experiments show that one of the most powerful benefits of \sys comes from the ensembling effects and its natural robustness to redundancy and/or distracting pipeline components.

