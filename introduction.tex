\section{Introduction}\label{intro}\sloppy
An important aspect of data science is handling dirty data including missing data, numerical outliers, syntactic errors, and integrity constraint violations.
Choosing how to handle a particular 




diverse forms


------and .





Data cleaning is super important, mor and more, it is a critical piece in tradiitonal applicatinos such as billing, hospital records, data collection, but also in modern data science and machine elarning pipelines.  It is arguably the major aspect of modern data science [cite our stuff].

The downstream applications that compute over the cleaned data are similarly diverse, and can be visualization aplications, ML, etc; this is hard because the appropriate way to clean the dataset for one application (e.g., describe billing) is different than another (e.g., improve a model).
These interdependencies and diversity mean that it is hard for data scientists to understand how to clean the data, which data to clean, and if using an automated cleaning tool, how to generalize its decisions to new datasets.

Related Work.  Cleaning solutions are siloed verticals.   Talk about defining what clean means is application specific (as setup for next paragraph).

In addition, there are several key limitations in current approaches.  (1) Siloed operation.   (2) custom cleaning algorithms make optimization and scaling challenging and one-off.  (3) cleaning algorithms directly update the dataset, so that the logic is opaque to the user and cannot be used for new datasets.  

In this paper, we argue that data cleaning is fundamentally an iterative human-in-the-loop process.
To best leverage the user's domain expertise, the cleaning system should enable the user to declaratively specify high level cleaning characteristics
such as what clean means for the application and the set of acceptable cleaning operations, and automatically propose cleaning programs to satisfy these high level goals.
This is similar to problems such as query synthesis, where users specify a target query output and the system generatesa  view, or program by example, where users specify input and output records and the system generates a string transformation program.

To this end, we present \sys, which models data cleaning as an optimization problem.  
Given a quality function that models the dirtiness of a relation, as well as parameterized cleaning operators that transform relations, \sys adopts a tree-based search algorithm motivated by the planning~\cite{} to search the space of cleaning programs for one that maximizes the quality function.
\ewu{describe a bit more}
This approach has many benefits.
First, many existing data cleaning algorithms such as FDs, statistics, etc~\cite{} can be modeled in terms of a quality function, and a pre-defined set of cleaning operators.
In fact, \sys enables these different notions of cleaning quality and cleaning operators to be combined within a single framework. \ewu{flesh out}
Second, the optimizations used in existing data cleaning algorithms can also be mapped to \sys as search optimizations. \ewu{flesh out}
Third, \sys is {\it simple}.  The core codebase is XXX lines of python code, and readily amenable to aggressive optimizations such as parallelization, search pruning, XXX, and other techniques used in the optimization and AI literature.
Finally, the cleaning programs generated by \sys can be used to clean new datasets.   This enables us to adopt measures from the machine leraning community, such as overfitting and generalization, to compare data cleaning approaches and provide a measure of cleaning robustness.

Beyond the novel problem formulation, the key technical challenge is to ensure that \sys can run quickly.  In general, the problem is APX-Hard, meaning that unless P=NP, there does not exist a polynomial time approximation.  However, recent progress in similar planning problems such as AlphaGo have shown that a prudent combination of XXX and YYY can sufficiently reduce the search space and generate high quality solutions (e.g., a sequence of game moves).   In our setting, we find that a combination of existing cleaning optimizations (e.g., blocking, approximation, etc), novel search pruning strategies that dynamically learns pruning rules, and XXX that exploits substantial structure in real-world datasets, can deliver runtime comparable to existing specialized systems while meeting or exceeding their cleaning accuracy.  

In our experiments, we find that \sys achieves parity with state-of-the-art constraint, statistical, and quantitative data cleaning systems in terms of accuracy.
On two datasets considered in prior data cleaning work of Flight arrival times and a Physician registry, \sys achieves a similar precision and recall to a recently proposed Denial Constraint system called HoloClean~\cite{rekatsinas2017holoclean}. 
Similarly, we applied \sys to numerical outlier problems and compared to the Minimum Covariance Determinant (MCD) algorithm, which is the basis of another recent system called Macrobase~\cite{bailis2016macrobase}.
Outliers removed by \sys  improved the accuracy of a downstream predictive models more significantly than MCD (5\% prediction accuracy improvement on the US Census Dataset) and a (4\% on an EEG dataset). 
Finally, we present initial results demonstrating the scaling properties of \sys where parallelism and caching can reduce search time by 77x.

In short, \sys is a new, simple architecture that models data cleaning as a search procedure and can combine quality measures and cleaning operations from diverse application domains.   This enables a cleaning system that directly leverages the user's domain expertise to articulate high level cleaning goals and search constraints.  Although it is still possible to design pathological settings for our iterative search-based approach, our experiments show strong evidence that \sys is comparable or better in terms of accuracy than specialized cleaning systems, can meet competitive run times to those systems, but easily extend to datasets with a mixture of error types---such as formatting errors and integrity constraint violations.   In addition, we believe the high level interface can accelerate end-to-end data cleaning for modern data science applications, while leveraging the substantial parallelization opportunities brought by the cloud.  \ewu{Describe what could in the future address the pathological issues?}





\if{0}
Data cleaning usually involves tedious manual specification of transformation rules.
For example, a data scientist might write a rule that maps every record with a \textsf{country} attribute ``United States'' to ``United States of America''.
These collections of rules quickly grow in size, and if they are developed in an ad-hoc way, they can be brittle and hard to maintain~\cite{krishnan2016hilda}.
Overly specific rules may not apply to future data, and overly general rules, might introduce unwanted side-effects.
Designing accurate transformation rules is a painstaking process, which is widely reported to be one of the most effort-intensive steps in data science~\cite{nytimes}.

We explore to what extent such transformation rules can be learned through a process of automatic trial-and-error on a dataset.
The system simulates different sequences of data transformations and scores the result according to a user-specified data quality objective function.
On its own, this is an inefficient way to search to search the set of transformation rules.
However, we can leverage Machine Learning to learn a strong pruning heuristic incrementally.
Transformations are often parametrized by literal values from the database (e.g., find string X and replace with string Y).
If we clean data in small partitioned blocks, we can incrementally build a classifier that infers common patterns in these literals, such as whether the strings tend to be similar or tend to be different or the attributes that are likely to be touched.
For example, consider the problem above where a data scientist is resolving inconsistencies in a \textsf{country} attribute to satisfy an integrity constraint.
As we iterate through the distinct \textsf{country} values and simulate possible replacement values, it might become clear that the source string have to be close to the target strings in string similarity, i.e., ``United States'' is more likely to be replaced by ``United States of America'' than ``Zimbabwe''.
Learning such a relationship automatically avoids hand coded heuristics which have to consider the complex relationship between how the analyst describes data quality and how exactly the transformations are defined.

This basic algorithm, optimizing a sequence of black-box transformations over a black-box data quality function, has a number of important advantages: (1) it is highly expressive as it can model a wide range of data cleaning formalisms from integrity constraints to quantitative data cleaning in the same framework, (2) it is relatively easy to parallelize the search, and (3) the result of the algorithm is not only a cleaned database instance but transformations that can be applied to future data.
This formalism casts data cleaning as a planning problem; analogous to the algorithms used AI, Robotic Planning, and Control.
Similar to the way one plans out a sequence of chess moves in AI to gain a strategic board position, we can think of data cleaning as planning out a sequence of data transformations to maximize the score on a data quality function.
And as in chess, where one cannot perfectly anticipate the opposing player's moves, in data cleaning we may not have a strong \emph{a priori} model of how a user-defined transformation rule modifies the data.
As a result, the algorithm and search heuristics cannot make strong assumptions about the anticipated structure of any search instance.
This sort of an approach is increasingly attractive because recent results in AI demonstrate scaling these search problems to  high branching-factor domains even when few assumptions are made.
Recent algorithms have been shown to match or exceed human performance in domains such as Go~\cite{silver2016mastering} and in Atari video games~\cite{mnih2015human}.
As in many classical data cleaning problems, an optimal solution to AI search problems is very hard to discover, but pragmatically leverage distributed computing and pruning rules learned from data can tractably find acceptable solutions.

We use this algorithm as a starting point for a new data cleaning system called \sys.
We designed an API that takes classical data cleaning problem specifications, such as integrity constraints, gold-standard manually cleaned data, and statistical models, and translates those specifications into an iterative learning search problem. 
Of course, we often need special-case optimizations that prune unproductive search branches to make the runtime more competitive with special case systems.
We provide a model where pruning rules are specified as regular expressions over a formal language of transformations and can be static (i.e., fixed before execution) and dynamic (i.e., inferred from properties of the dirty database instance).
The search algorithm we use is a memory-bounded best-first search which maintains the subset of the search frontier that can fit in memory. 
This algorithm can be parallelized, distributed, and can cache repeated computations.
\fi



