\section{Introduction}\label{intro}\sloppy

It is widely known that data cleaning is one of the most time-consuming steps of the data analysis process~\cite{nytimes}, and designing algorithms and systems to automate or partially automate data cleaning continues to be an active area of research~\cite{DBLP:conf/sigmod/ChuIKW16}.  Automated data cleaning takes as input a {\it specification} that characterizes the errors in the dataset, as well as a set of allowable {\it data transformations} (e.g., \texttt{set(rowi, attrj, val)}), and automatically applies these transformations to fix the errors.  

Although this is a broad description, most existing automated data cleaning approaches develop optimized systems for specific classes of data errors.  For example, functional dependencies (FDs) specify errors as records that violate the FD constraint, and data cleaning engines typically transform the dataset by deleting the violating records~\cite{}, or setting attribute values to resolve the constraints~\cite{}.  Alternatively, \ewu{DESCRIBE ANOTHER CLASS SUCH AS STRING EXTRACTION}.  There is a rich ecosystem of data cleaning libraries, tools, and systems for specific domains and error types.

However, automated data cleaning data scientists and modern data analysis is particularly challenging due to several trends.  

% However, automation in data cleaning is challenging because real-world data is highly variable.  A single data set can have many different types of data corruption such as statistical outliers, constraint violations, and duplicates.  Once an error is detected, there is a further question of how to repair this error, which often depends on how the data will be used in the future.  


First, data is increasingly collected across many different data sources; each dataset can exhibit different types of errors that are domain specific.  These error types can be {\it syntactic}, due to e.g., data extraction bugs that cause multiple attributes to be concatenated into a single string, or schema detection errors that cause entire columns to be shifted.  They can also be {\it semantic}, such as incorrect attribute values, functional dependency violations, or sequential data that does not conform to an expected shape.   Each type of error, in each domain, often requires different methods to fix them.  These can range from complex imputation algorithms that use domain-specific error models (ref signal processing and holoclean), to simple rules that are easy to report and understand~\cite{}.  \ewu{For each, there are numerous specialized solutions}

Second, datasets often contain mixtures of the above errors.  These errors may be introduced at any part of the data extraction, cleaning, and processing pipeline before the devolper analyzes it.  For instance, data entry errors or incorrect survey coding may introduce semantically incorrect values for some attribute values.  In addition, data extraction or schema alignment algorithms may introduce syntactic errors where attributes are shifted or their values are concatenated.  Further, these errors are not independent---improper extraction affects the ability to interpret semantic values.  

Third, developers and users often do not know the quality characteristics and constraints of their data up-front.  Instead, they often start with vague notions of what signals are associated with high quality data, and iteratively refine their understanding throughout the data cleaning process~\cite{}.  Unfortunately, the need to rapidly test quality measures is hampered by the need to manually compose and ``glue'' disparate data cleaning operations and systems together in order to test each hypothesis.  

% \ewu{human interfaces with quality metrics, mechanisms are data transformations, alpha clean works to combine these together.}

Ultimately, data cleaning is a human-in-the-loop process driven by the user's domain expertise.  Thus, the input and output interfaces should be flexible and optimized for the user.   Users should be able to easily specify, and rapidly refine, potentially arbitrary {\it data-quality measures}.  It should also be easy to pick data transformation operators from a pre-existing library, or wrap specialized data cleaning libraries as user-defined operators.  To ensure that the system output is easy to understand, the system should generate simple programs composed of the user-specific transformation operators, rather than directly fixing the database contents.

As a concrete example, consider the following example of addresses in Figure~\ref{f:example-data}.  \reminder{SANJAY DESCRIBE ITS DATA QUALITY ISSUES.  Emphasize that the quality issues are complex, learned during the cleaning process, and cannot be captured by simple FD-style constraints.  }

This examples shows that, although specialized data cleaning systems can find fixes for a subset of errors, no system can generate a program to fully clean the dataset.  

\stitle{A Simple, General System}
In this paper, we take a step back from the specialized systems approach, and ask if a simple, general meta-algorithm can be used to address common data cleaning problems.  Our primary hypothesis is that data cleaning errors are typically highly structured, and a learning-based approach that can identify these structures for different cleaning problems.   
Data errors are often systematic where they are correlated with with certain attributes and values in the dataset~\cite{rekatsinas2017holoclean,DBLP:journals/pvldb/KrishnanWWFG16}.
Consequently, as more data is cleaned, we can better identify common patterns to prioritize the search on future data.

To this end, we design and evaluate \sys, which employs a simple tree search algorithm that finds a sequence of data transformation operations to maximize a user-specified data quality measure.   The user provides parameterized operators and the system identifiez the specific parameterizations throughout the search process.  In our experiments, we show how external data cleaning libraries such as \ewu{XXX and YYY} can be wrapped as parameterized black-boxes.  Similarly, the user can incrementally add additional constraints and conditions to the data quality measure.  These can be expressed as generic Python functions over a dataset, and we describe optimizations based on the distributive or algebraic properties of commen quality measures that capture e.g, entity resolution, functional dependencies.

As \sys searches possible programs by executing candidate operations and computing the resulting quality measure, it learns a prediction model to estimate the expected improvement of different transformation operators to quickly prune the search space.  In our work, we show that a simple linear classifier is effective for a wide variety of data cleaning datasets used in the current literature, however more complex prediction models such as neural networks~\cite{} may be used for more complex or widely used problems that can generate lots of training data.   A benefit of learning these models is that \sys can bootstrap new data cleaning problems by using previously learned models; we show how \sys can more quickly generate data cleaning programs when users can iteratively refine their data quality measures.

The highlight of a simple search algorithm is that it is flexible to the optimization criteria and the library of transformation operators, and can benefit from the considerable recent advances in planning optimization~\cite{silver2016mastering}. We illustrate one such advance by leveraging modern cloud and GPU infrastructure to cheaply parallelize the search algorithm.  Our experiments show near-linear scalability as the level of parallelism increases, and even faster when using learned rules for pruning.

Finally, one concern is that a generic optimization algorithm may cripple the data-cleaning quality.  To understand this, we evaluated \sys on \ewu{N} existing data-cleaning benchmarks, and find that \sys performs comparably in accuracy and runtime with recently reported results from state-of-the-art systems.  

% Finally, the highlight of \sys is its flexibility to the variety and combination of data error types and cleaning transformation.  


% Across 8 real-world datasets used in prior data cleaning literature, we show that \sys matches or exceeds the cleaning accuracy and exhibits competitive run-times to state-of-the-art approaches that are specialized to a specific error domain (constraint, statistical, or quantitative errors).  



\noindent To summarize, our contributions include:
\begin{itemize}[leftmargin=*, topsep=0mm, itemsep=0mm]
  \item The design and evaluation of a general search-based approach to data cleaning, that combines automated cleaning for arbitrary combinations of syntactic and semantic data errors.
  \item The development of decomposable data cleaning measures that are amenable to parallel execution.
  \item A suite of pragmatic optimizations, such as fast pruning using predictive models, multi-node parallelization, and data sharing to reduce network communication bottlenecks, that reduce the runtime by \ewu{XXX$\times$}.
  \item A systematic study of the benefits and limitations of \sys in terms of data cleaning accuracy (precision, recall), and the runtime.  We show that \sys can solve incremental refinements of the data quality measure $yyy\times$ faster than from scratch, and that \ewu{SOME OTHER FINDING}
  % \item Finally, we show that \sys can solve existing data cleaning benchmarks at competitive runtimes and accuracies as existing specialized data cleaning systems.  We believe this is not necessarily due to superiority of \sys, but may be a symptom of limitations of existing cleaning benchmarks, which have simple structure.
\end{itemize}


