\section{Introduction}\label{intro}\sloppy

intro:
pander to databases: 
objective?  downstream quality function
what types of pipelines? how to model?
why not the closest hyperparameter search?

naive 1: AutoML
most param settings is bad.  want to prune those out.
very delayed feedback because need to actually execute the pipeline before seeing results (and executing the pipeline can be very expensive)

naive 2: program synthesis
explotion in branching factor by allowing all possible transforms
broader objective function class than foofah

mixture of simple operators.  Not learning a language
experiments are too simple.  

* there is no transformation holoclean can do that we cannot do
* strip things down to clearly focus on the search algorithm 
* the transforms are actually simple in any system, it's how they are selected and parameterized that is challenging.  

we help you compose the objectives using a simple API that simply prioritizes features correlated with the error


It is widely known that data cleaning is one of the most time-consuming steps of the data analysis process~\cite{nytimes}, where an analyst may spend upwards of 80\% of the analysis effort on cleaning and prepper the data.  A core challenge in data cleaning is that the solutions are incredibly domain and application-specific, and the analyst must explore a massive space of possible errors {\it and} data cleaning operations in order to identify the appropriate sequence of data cleaning transformations that is appropriate for her problem.

Consider a single logical cleaning operation such as addressing outlier values.  There is a large space of physical data cleaning operations: simply detecting the outliers could utilize a fixed threshold, an unsupervised statistical model, a problem-specific model (e.g., of EEG signals~\cite{}), a based on constraints.  Each operation further burdens the developer with numerous choices to set its parameters---what threshold?  How many clusters? What model features?   Once the outliers have been detected, there is a similar buffet of options to actually replace the outlier value.     Unfortunately, data cleaning is challenging because even a single data set can have many different types of errors (e.g., outliers, constraint violations, duplicates, mis-spellings, incorrect orderings).

To address this problem in practice, organizations often curate a library of common implementations of data cleaning operations (e.g., a set of EEGspecific outlier detectors) and develop data cleaning pipelines to more easily combine them into data cleaning pipelines~\cite{krishnan2016hilda}.  Similarly, a number of declarative data cleaning frameworks~\cite{DBLP:journals/pvldb/HaasKWF015,gokhale2014corleone,stonebraker2013data,giannakopoulou2017cleanm} provide high-level APIs and languages to address broad classes of data cleaning problems.  Although these solutions help address the engineering and execution aspects to complex data cleaning problems, the developer is still faced, at each step, with numerous choices in terms of the operator to use, how to set its parameters, and the order to compose the operators.  There is a need to develop \textbf{automated approaches to identify, parameterize, and compose data cleaning operators to satisfy the user's cleaning goal}.  Unfortunately, this is challenging for a number of reasons.

%There is a need to automate data cleaning~\cite{DBLP:conf/sigmod/ChuIKW16}


% data is increasingly collected across many different data sources; each dataset can exhibit different types of errors that are domain specific.

First, there can be a mixture of many error types.    These error types can be {\it syntactic}, due to e.g., data extraction bugs that cause multiple attributes to be concatenated into a single string, or schema detection errors that cause entire columns to be shifted.  They can also be {\it semantic}, such as incorrect attribute values, functional dependency violations, or sequential data that does not conform to an expected trend.   Each type of error, in each domain, often requires different methods to fix them.  These can range from complex imputation algorithms that use domain-specific error models (ref signal processing and holoclean), to simple rules that are easy to report and understand~\cite{}.  \ewu{For each, there are numerous specialized solutions}

Second, datasets often contain mixtures of the above errors.  These errors may be introduced at any part of the data extraction, cleaning, and processing pipeline before the devolper analyzes it.  For instance, data entry errors or incorrect survey coding may introduce semantically incorrect values for some attribute values.  In addition, data extraction or schema alignment algorithms may introduce syntactic errors where attributes are shifted or their values are concatenated.  These errors are not independent. For instance, the above concatenation error affects the ability to interpret semantic values.  

Most importantly, developers and users often do not know the quality characteristics and constraints of their data up-front.  Instead, they examine their analysis output to develop hypotheses about the presence and type of errors in their data; they then iteratively refine their understanding throughout the data cleaning process~\cite{krishnan2016hilda}.  The developer then needs to try different incantations of cleaning operators to test her hypothesis.   However, the need for the developer to reason about the application-level data error characteristics {\it and} manually explore the physical operator space impedes the end-to-end cleaning process.   

Ultimately, data cleaning is a human-in-the-loop process driven by the user's domain expertise.  Thus, the input and output interfaces should be flexible and optimized for the user.   Users should be able to easily specify, and rapidly refine, potentially arbitrary {\it data-quality measures}.  It should also be easy to pick data transformation operators from a pre-existing library, or wrap specialized data cleaning libraries as user-defined operators.  To ensure that the system output is easy to understand, the system should generate simple programs composed of the user-specific transformation operators, rather than directly fixing the database contents.

A naive approach is to layer a black-box hyper-parameter tuning algorithm on an existing data cleaning framework.  The selection of operators, how they are combined, and how their parameters are set, can be modeled as a separate hyperparemeter that an optimization algorithm such as X, Y, or Z will explore.  The user's task is to specify and tune the optimization objective to lead the search towards an acceptable solution.   \ewu{THere's a gap here that I'm missing}

However this approach ignores important aspects of data cleaning problems that enable more efficient and flexible approaches.  First, data cleaning programs are often a sequence of transformations applied one after the other, thus the entire space of hyperparameters does not need to be learned jointly.  Second, a given transformation may be applied multiple times, whereas a black-box hyperparameter search algorithm would need to know up front the maximum set of trnsaforamiotns and the parameters. Third, the transformations are not commutative, and prior transformations can affect subsequent transformations.  This meoans that local decisions need to account for how they may affect later transformations.  


%Automated data cleaning takes as input a {\it specification} that characterizes the errors in the dataset, as well as a set of allowable {\it data transformations} (e.g., \texttt{set(rowi, attrj, val)}), and automatically applies these transformations to fix the errors.  

% Data  cleaning is composed of pipelines.  in some cases, the pipelines are largely fixed, and developers adjust the parameters of operators in the pipeline, or the pipeline itself, in response to changes in the input data.  In other cases, the core set of parameterized operators are know apriori, but the specific parameterization and pipeline of operators that should be run to achieve a desired data cleaning goal is not clear up front. The latter can happen at a regular basis as new data cleaning scenarios arise (say, due to new customers), a new data source is added to an analysis, or when a data source changes substantially.  In these cases, developers must construct the pipeline and parameterization in order to achieve an often ill-defined cleaning goal.


\stitle{A Simple, General Approach}
In this paper, we take a step back from the specialized systems approach, and ask if a simple, general meta-algorithm can be used to address common data cleaning problems.  Our primary hypothesis is that data cleaning errors are typically highly structured, and a learning-based approach that can identify these structures for different cleaning problems.   
Data errors are often systematic where they are correlated with with certain attributes and values in the dataset~\cite{rekatsinas2017holoclean,DBLP:journals/pvldb/KrishnanWWFG16}.
Consequently, as more data is cleaned, we can better identify common patterns to prioritize the search on future data.




To this end, we design and evaluate \sys, which employs a simple tree search algorithm that finds a sequence of data transformation operations to maximize a user-specified data quality measure.   The user provides parameterized operators and the system identifiez the specific parameterizations throughout the search process.  In our experiments, we show how external data cleaning libraries such as \ewu{XXX and YYY} can be wrapped as parameterized black-boxes.  Similarly, the user can incrementally add additional constraints and conditions to the data quality measure.  These can be expressed as generic Python functions over a dataset, and we describe optimizations based on the distributive or algebraic properties of commen quality measures that capture e.g, entity resolution, functional dependencies.

As \sys searches possible programs by executing candidate operations and computing the resulting quality measure, it learns a prediction model to estimate the expected improvement of different transformation operators to quickly prune the search space.  In our work, we show that a simple linear classifier is effective for a wide variety of data cleaning datasets used in the current literature, however more complex prediction models such as neural networks~\cite{} may be used for more complex or widely used problems that can generate lots of training data.   A benefit of learning these models is that \sys can bootstrap new data cleaning problems by using previously learned models; we show how \sys can more quickly generate data cleaning programs when users can iteratively refine their data quality measures.

The highlight of a simple search algorithm is that it is flexible to the optimization criteria and the library of transformation operators, and can benefit from the considerable recent advances in planning optimization~\cite{silver2016mastering}. We illustrate one such advance by leveraging modern cloud and GPU infrastructure to cheaply parallelize the search algorithm.  Our experiments show near-linear scalability as the level of parallelism increases, and even faster when using learned rules for pruning.

Finally, one concern is that a generic optimization algorithm may cripple the data-cleaning quality.  To understand this, we evaluated \sys on \ewu{N} existing data-cleaning benchmarks, and find that \sys performs comparably in accuracy and runtime with recently reported results from state-of-the-art systems.  

% Finally, the highlight of \sys is its flexibility to the variety and combination of data error types and cleaning transformation.  


% Across 8 real-world datasets used in prior data cleaning literature, we show that \sys matches or exceeds the cleaning accuracy and exhibits competitive run-times to state-of-the-art approaches that are specialized to a specific error domain (constraint, statistical, or quantitative errors).  



\noindent To summarize, our contributions include:
\begin{itemize}[leftmargin=*, topsep=0mm, itemsep=0mm]
  \item The design and evaluation of a general search-based approach to data cleaning, that combines automated cleaning for arbitrary combinations of syntactic and semantic data errors.
  \item The development of decomposable data cleaning measures that are amenable to parallel execution.
  \item A suite of pragmatic optimizations, such as fast pruning using predictive models, multi-node parallelization, and data sharing to reduce network communication bottlenecks, that reduce the runtime by \ewu{XXX$\times$}.
  \item A systematic study of the benefits and limitations of \sys in terms of data cleaning accuracy (precision, recall), and the runtime.  We show that \sys can solve incremental refinements of the data quality measure $yyy\times$ faster than from scratch, and that \ewu{SOME OTHER FINDING}
  % \item Finally, we show that \sys can solve existing data cleaning benchmarks at competitive runtimes and accuracies as existing specialized data cleaning systems.  We believe this is not necessarily due to superiority of \sys, but may be a symptom of limitations of existing cleaning benchmarks, which have simple structure.
\end{itemize}


