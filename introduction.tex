\section{Introduction}\label{intro}\sloppy
It is widely known that data cleaning is one of the most time-consuming steps of the data analysis process~\cite{nytimes}, and
designing algorithms and systems to automate or partially automate data cleaning continues to be an active area of research~\cite{DBLP:conf/sigmod/ChuIKW16}.
Automation in data cleaning is challenging because real-world data is highly variable. 
A single data set can have many different types of data corruption such as statistical outliers, constraint violations, and duplicates.
Once an error is detected, there is a further question of how to repair this error, which often depends on how the data will be used in the future.

This variability creates a tension between the needs of the data scientist and the designer of the data cleaning framework.
While the data scientist might desire a single cleaning framework that addresses all of her data errors and repair operations, from an algorithmic perspective, it is far more efficient to consider more restricted models.
Data cleaning tools are often highly optimized for particular problems  (e.g., see statistical outliers~\cite{hellerstein2008quantitative}, enforcing logical constraints~\cite{DBLP:conf/sigmod/ChuIKW16}, entity resolution~\cite{DBLP:journals/pvldb/KopckeTR10}). 
Consequently, a recent survey of industry suggests that data cleaning pipelines are often a patchwork of custom scripts and multiple specialized systems~\cite{krishnan2016hilda}.
The overhead to setup, learn, and manage multiple cleaning systems can easily outweigh their benefits.
Some data scientists eschew automated tools altogether and simply write data cleaning programs from scratch.
The customized programming approach quickly becomes difficult to mantain and interpret; especially for users without data engineering expertise~\cite{sculley2014machine}.

With the availability of vastly more distributed computing resources, perhaps, the focus on restricted data cleaning models should be reconsidered.
Recent results in AI, such as AlphaGo~\cite{silver2016mastering}, show that a combination of Machine Learning and massively parallelized search can effectively optimize very complex and general objective functions.
Data cleaning is very similar to the planning problems considered in AI.
In planning, the system is given a state space and actions that transition between states, and generates a sequence of actions (a plan) that maximizes a quality function. The classic application is a computer chess program that must plan a sequence of chess moves that change the state of the board in order to maximize the likelihood of a checkmate. Likewise, in data cleaning, one is given a dirty relation and a way to measure data quality (e.g., number of integrity constraints violated), and the data cleaning problem is to find a sequence of modifications to the relation that maximize the data quality metric.

Most non-trivial planning problems are very hard without good search heuristics.
Advances in machine learning have made it possible to replace hand-crafted heuristics with automatically learned pruning functions that estimate the expected quality of candidate plans. AlphaGo learned pruning heuristics using a neural network trained on data generated through self-play~\cite{silver2016mastering}. 
The key insight is that expertly written heuristics can be replaced with data-driven models, which makes the techniques applicable to a diverse set of applications.   
This insight is highly relevant to data cleaning, where datasets tend to have structure amenable to learning heuristics adaptively.
Data errors are often systematic where they are correlated with with certain attributes and values in the dataset~\cite{rekatsinas2017holoclean,DBLP:journals/pvldb/KrishnanWWFG16}.
Consequently, as more data is cleaned, we can often quickly identify common patterns, which can help prioritize the search on future data.

\sys is a new data cleaning system that is designed around an algorithmic planning architecture. It takes as input a continuous-valued {\it quality function} that models the data quality of a relation and a {\it language} of parameterized data cleaning operators, and outputs a sequence of data cleaning transformations (a cleaning program) from the language that seeks to maximize the quality function.  This API imposes minimal restrictions on the quality function, and we show that it can express arbitrary combinations of existing data cleaning formalisms such as functional dependencies, denial constraints, text formatting, and statistical models.  
\sys can also encode problem-specific optimizations as search pruning rules (e.g., disallowed transformation sequences) or modifications to the data representation (e.g., clustering similar records).  
In addition to hard-coded pruning rules, \sys can adaptively learn to prioritize the promising subsets of the search space. 
While AlphaGo used a deep neural network to model the heurisitic, we found that a logistic regression classifier was sufficient. 
This approach has a number of benefits: (1) users can focus on expressing their application's notion of data quality using the expressive quality function, (2) the output is a transformation sequence rather than a clean database instance that is more interpretable and can be applied to new datasets, and (3) the core algorithm is both simple  ($<500$ lines of Python) and general across different data error domains and problem instances.

 \sys uses a best-first search that greedily appends data transformations to the set of best candidate programs seen so far, and adopts parallelization and pruning ideas from the planning literature.  In contrast to traditional planning, where the search state (e.g., chess board) is compact and largely trivial to parallelize in a distributed setting, the data cleaning search state is the size of the input dataset and introduces a trade-off between communication costs to share intermediate state and the degree of parallelism possible.  We develop a gather-scatter style method to parallelize \sys in a distributed context. 
 
It is natural to expect that such a general formulation must sacrifice cleaning accuracy, runtime performance, or both.  Our primary technical contribution is to show that neither need be sacrificed. 
It is important to acknowledge that \sys loses many of the provable guarantees provided by specialized systems based on logical constraints, and is in some sense, best-effort.  
Across 8 real-world datasets used in prior data cleaning literature, we show that \sys matches or exceeds the cleaning accuracy and exhibits competitive run-times (provided enough computational resources are available) to state-of-the-art approaches that are specialized to a specific error domain (constraint, statistical, or quantitative errors).  At the same time, \sys generates an interpretable cleaning program that be re-used on new data, and can clean datasets that exhibit errors spanning multiple error domains. 

\sys highlights an approach to data cleaning where users articulate high level cleaning goals, and the system automatically proposes an interpretable cleaning program.  It is a step towards a unified framework for automatic data cleaning systems, similar in spirit to the relational model, which combined transaction, analytic, graph, machine learning, and other data-intensive applications under a unified specification, optimization, and execution framework.

















\if{0}
Data cleaning usually involves tedious manual specification of transformation rules.
For example, a data scientist might write a rule that maps every record with a \textsf{country} attribute ``United States'' to ``United States of America''.
These collections of rules quickly grow in size, and if they are developed in an ad-hoc way, they can be brittle and hard to maintain~\cite{krishnan2016hilda}.
Overly specific rules may not apply to future data, and overly general rules, might introduce unwanted side-effects.
Designing accurate transformation rules is a painstaking process, which is widely reported to be one of the most effort-intensive steps in data science~\cite{nytimes}.

We explore to what extent such transformation rules can be learned through a process of automatic trial-and-error on a dataset.
The system simulates different sequences of data transformations and scores the result according to a user-specified data quality objective function.
On its own, this is an inefficient way to search to search the set of transformation rules.
However, we can leverage Machine Learning to learn a strong pruning heuristic incrementally.
Transformations are often parametrized by literal values from the database (e.g., find string X and replace with string Y).
If we clean data in small partitioned blocks, we can incrementally build a classifier that infers common patterns in these literals, such as whether the strings tend to be similar or tend to be different or the attributes that are likely to be touched.
For example, consider the problem above where a data scientist is resolving inconsistencies in a \textsf{country} attribute to satisfy an integrity constraint.
As we iterate through the distinct \textsf{country} values and simulate possible replacement values, it might become clear that the source string have to be close to the target strings in string similarity, i.e., ``United States'' is more likely to be replaced by ``United States of America'' than ``Zimbabwe''.
Learning such a relationship automatically avoids hand coded heuristics which have to consider the complex relationship between how the analyst describes data quality and how exactly the transformations are defined.

This basic algorithm, optimizing a sequence of black-box transformations over a black-box data quality function, has a number of important advantages: (1) it is highly expressive as it can model a wide range of data cleaning formalisms from integrity constraints to quantitative data cleaning in the same framework, (2) it is relatively easy to parallelize the search, and (3) the result of the algorithm is not only a cleaned database instance but transformations that can be applied to future data.
This formalism casts data cleaning as a planning problem; analogous to the algorithms used AI, Robotic Planning, and Control.
Similar to the way one plans out a sequence of chess moves in AI to gain a strategic board position, we can think of data cleaning as planning out a sequence of data transformations to maximize the score on a data quality function.
And as in chess, where one cannot perfectly anticipate the opposing player's moves, in data cleaning we may not have a strong \emph{a priori} model of how a user-defined transformation rule modifies the data.
As a result, the algorithm and search heuristics cannot make strong assumptions about the anticipated structure of any search instance.
This sort of an approach is increasingly attractive because recent results in AI demonstrate scaling these search problems to  high branching-factor domains even when few assumptions are made.
Recent algorithms have been shown to match or exceed human performance in domains such as Go~\cite{silver2016mastering} and in Atari video games~\cite{mnih2015human}.
As in many classical data cleaning problems, an optimal solution to AI search problems is very hard to discover, but pragmatically leverage distributed computing and pruning rules learned from data can tractably find acceptable solutions.

We use this algorithm as a starting point for a new data cleaning system called \sys.
We designed an API that takes classical data cleaning problem specifications, such as integrity constraints, gold-standard manually cleaned data, and statistical models, and translates those specifications into an iterative learning search problem. 
Of course, we often need special-case optimizations that prune unproductive search branches to make the runtime more competitive with special case systems.
We provide a model where pruning rules are specified as regular expressions over a formal language of transformations and can be static (i.e., fixed before execution) and dynamic (i.e., inferred from properties of the dirty database instance).
The search algorithm we use is a memory-bounded best-first search which maintains the subset of the search frontier that can fit in memory. 
This algorithm can be parallelized, distributed, and can cache repeated computations.
\fi



\if{0}


view data cleaning from a planning~\cite{} and optimization perspective, and show evidence that a simple, general approach to this problem can provide the user with considerable flexibility in terms of cleaning data with different classes of errors, controlling the allowable operations, and customizing the system to new types of data errors.



Ideally, the majority of the data cleaning process should reside in a single system with a  \emph{declarative} interface, wherein the analyst specifies a high-level data model (e.g., constraints the data should satisfy) and a system automatically generates the pipeline to enforce the model.
The idea of declarative data cleaning is not new~\cite{rahm2000data}, but such approaches have classically been restricted to data models specified in subsets of first-order logic and only recently have considered extensions better handle uncertain numerical data~\cite{prokoshyna2015combining}.
The prevailing wisdom is that there is an inherent tradeoff between the expressiveness of the data model and the efficiency of the (approximate) solution algorithm---limited models allow the system designer to exploit specific algorithmic structures.


However, two recent trends, namely, the success of model-free learning in AI and the availability commodity cloud computing encourage us to reconsider this algorithmic philosophy.
Recent progress in planning problems such as AlphaGo~\cite{silver2016mastering} and automatically playing Atari video games~\cite{mnih2015human} have shown that a prudent combination of Machine Learning and distributed search can approximately optimize very complex black-box objective functions.
The intriguing aspect of these results is that the same algorithm, called Deep Q Reinforcement Learning, that learns to play an Atari game~\cite{mnih2015human} can be used on a very different problem, such as training a robot~\cite{gu2017deep}.
The underlying optimization algorithm encodes few specifics about the objective or structure of the domain  and pragmatically finds a reasonable local optimum.
The optimization algorithm is general enough to support a very wide class of problems--at the cost of additional computational resources since it is unaware of instance-specific structure.

\fi
