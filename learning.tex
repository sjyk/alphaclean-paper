\section{Learning Pruning Rules}\label{s:pruning}
Data errors are often systematic in nature where the correlate with specific attribute values.
\sys can exploit such correlations in the dataset to adaptively prune the search space.
The basic strategy is to apply the search independently to different partitions of the dataset.
\sys can exploit shared structure between these data partitions by learning pruning rules.

\subsection{Partitioning A Dataset}
Many large datasets are naturally partitioned, e.g., by timestamp or region. 
The idea is to partition the dataset in such a way that errors are local to a small number of records.
This means that a fix for a given record does not affect the other records outside of the partition.
There is a relationship between partitioning and the quality functions defined.
For example, quality functions derived from functional dependencies can define blocks by examining the violating tuples linked through the dependency.  Similarly, users can define custom partitioning functions.  In our current implementation, we partition the input relation by row by user-specified blocking rules.

\subsection{Overview and Intuition}\label{s:dynlearn}
The main idea is to develop search pruning rules to minimize the number of candidate programs to evaluate.  Rather than hand-write pruning heuristics {\it a priori}, which is challenging when the user is presented with a new dataset, this optimization leverages the problem structure to learn pruning rules during the search process.
The search algorithms in  most automatic data cleaning frameworks are carefully tuned for a specific quality function or class of quality functions. For example, the chase used in functional dependency resolution does not make an edit to the table unless it enforces at least one tuple's FD relationship.    Similarly, in entity matching problems, one restricts the search to only matching tuples that are likely to be similar based on some similarity metric.
These can be viewed as search pre-conditions that exploit the structure of the cleaning problem to a more tractable search space.
Note that these are not static optimizations: knowing how to prune the search space requires identifying and modeling the structure of the underlying data errors and how they interact with the data transformations.
% the underlying data or making strong modeling assumptions about the types of transformations used.


\subsection{Approach}
When \sys executes the search algorithm on each block of data, it generates a cleaning program that optimizes the quality metric for that block.  In many cases, the dataset can be partitioned into a large number of blocks that each serve as sources of training examples for a learned pruning model.  Each transformation in a block's  cleaning plan $s$ can be labeled as a positive training example, while all other transformations serve as negative examples.
As \sys processes more blocks, the union of these training sets can be sufficient to train a classifier to predict whether a given transformation will be included in the optimal program.  In our approach, the prediction model $M(p): P \mapsto \{0,1\}$ is over the data transformations and not the data; is this sense, \sys learns pruning rules in a dynamic fashion. 
New expansions are tested against the classifier before the algorithm proceeds.
Internally, \sys uses a Logistic Regression classifier that is biased towards false positives (i.e., keeping a bad search branch) over false negatives (e.g., pruning a good branch). This is done by training the model and shifting the prediction threshold until there are no False Negatives. 

\subsection{Featurization}
Note to use this approach, we have to featurize each conditional assignment $c$ into a feature vector.
We \emph{do not} featurize the data as in other learning-based data cleaning systems.
Now, we describe how each conditional assignment is described as a feature vector.
Let $A$ be a list of all of the attributes in the table in some ordering (e.g., $[city\_name, city\_code]$).
Every conditional assignment statement is described with a predicate \texttt{pred}, \texttt{targ} a target attribute, and a target \texttt{value}. 
$A_{pred}$ is the subset of attributes that satisfy the predicate and $A_{target}$ is the singleton set representing the target attribute.
Each of these sets can be turned into a $|A|$-dimensional binary vector, where 1 represents presence of an attribute, and we call these vector $f_{pred}$ and $f_{target}$ respectively.
Then, we include information about the provenance of the conditional assignment $c$, from which data cleaning method it was generated and what parameter settings.
This feature called $f_{dc}$ is contains a 1-hot feature vector describing which is the source data cleaning method and any numerical parameters from the source method. 

We believe this is one of the reasons why a simple best-first search strategy can be effective.  For the initial blocks, \sys searches without a learned pruning rule in order to gather evidence.  Over time, the classifier can identify systematic patterns that are unlikely to lead to the final cleaning program, and explore the rest of the space.  
The features guide \sys towards those data cleaning methods/parameter settings that are most promising on previous blocks.
 \sys uses linear classifier and feature extractors because it can be trained quickly with few examples.   However, we speculate that across a sufficient number of cleaning problems that share common set of data transformations (say, within the same department), we may adapt a deep learning approach to automatically learn the features themselves. 
 
 \subsection{Discussion}
This incremental learning process can be viewed as form of reinforcement learning.
\textbf{TODO}









\iffalse
\stitle{Faster Data Quality Refinement}
\ewu{I emphasize this assuming the experiment exists!!!}
One benefit of learning a pruning model for {\it data transformations} rather than relation instances is that it can potentially be reused or fine-tuned for new, but structurally similar, data cleaning problems.  For instance, when users iteratively refine the data quality measure, the problem is largely the same, and \sys can intialize the search with the previously learned model.  This has the potential to learn  {\it across data cleaning problems} rather than across blocks within a single problem.   \ewu{We evaluate the benefits of re-using models during iterative refinement in Section~\ref{XXX}.}
\fi


 

% For example, some columns might not be dirty and are not worthwile to clean.  In the example above, another observation could be that the source and target strings in the optimal sequence are very close in terms of string similarity (as opposed to arbitrary transformations).  If each of these operations was featurized with a single scalar that is the edit-distance between the two strings, then the classifier could learn a pruning threshold (i.e., not considering find-and-replace operations above that threshold).  

% Consider an alternative to a predefined search heuristic where we clean data in small blocks.  For the initial blocks, we search without a heuristic.  As we continuously perform the search, we train a classifier on these features to reject search branches that are not typically in the final solution.  This allows us to exploit any patterns in the literal parameters that repeatedly occur.






