\subsection{Learning Pruning Rules}\label{s:pruning}

Traditionally, data cleaning systems manually implement pruning heuristics for a fixed quality function that can work for any dataset. For example, the Chase~\cite{} used in functional dependency resolution does not make an edit to the table unless it enforces at least one tuple's FD relationship.  Similarly, in entity matching problems, one restricts the search to only tuples that satisfy a blocking (clustering) criteria.  These can be viewed as pre-conditions over the search space.

Our idea is to {\it learn} pruning rules that are data and quality function dependent.  Our hypothesis is that data errors are often systematic in nature, and correlated with specific attributes and their values.  Our pruning optimization seeks to distinguish conditional assignments that are likely to contribute to the final cleaning pipeline, and those that will not.  To do so, the basic strategy is to independently execute the Search algorithm on partitions of the dataset, and learn a prediction model.   

% Note that these are not static optimizations: knowing how to prune the search space requires identifying and modeling the structure of the underlying data errors and how they interact with the data transformations.



\stitle{Approach}
As described in Section~\ref{s:search}, \sys uses data parallelism to execute search for each block of the dataset in parallel.  Thus, each block results in an optimal cleaning pipeline.  \sys models the optimal cleaning pipeline for each block as a set of training examples.  Specifically, cach conditional assignment $c$ in a block's optimal cleaning plan $s*$ can be labeled as a positive training example, while all other conditional assignments that were not used are negative examples.  

As \sys processes more blocks, it trains a classifier to predict whether a given transformation will be included in the optimal program, based on the training examples across the blocks.  In our approach, the prediction model $M(p): P \mapsto \{0,1\}$ is over the data transformations and not the data; is this sense, \sys learns pruning rules in a dynamic fashion. 
New expansions are tested against the classifier before the algorithm proceeds.
Internally, \sys uses a Logistic Regression classifier that is biased towards false positives (i.e., keeping a bad search branch) over false negatives (e.g., pruning a good branch). This is done by training the model and shifting the prediction threshold until there are no False Negatives. 

\stitle{Featurization}
Note to use this approach, we have to featurize each conditional assignment $c$ into a feature vector.
We \emph{do not} featurize the data as in other learning-based data cleaning systems.
Now, we describe how each conditional assignment is described as a feature vector.
Let $A$ be a list of all of the attributes in the table in some ordering (e.g., $[city\_name, city\_code]$).
Every conditional assignment statement is described with a predicate \texttt{pred}, \texttt{targ} a target attribute, and a target \texttt{value}. 
$A_{pred}$ is the subset of attributes that satisfy the predicate and $A_{target}$ is the singleton set representing the target attribute.
Each of these sets can be turned into a $|A|$-dimensional binary vector, where 1 represents presence of an attribute, and we call these vector $f_{pred}$ and $f_{target}$ respectively.
Then, we include information about the provenance of the conditional assignment $c$, from which data cleaning method it was generated and what parameter settings.
This feature called $f_{dc}$ is contains a 1-hot feature vector describing which is the source data cleaning method and any numerical parameters from the source method. 

We believe this is one of the reasons why a simple best-first search strategy can be effective.  For the initial blocks, \sys searches without a learned pruning rule in order to gather evidence.  Over time, the classifier can identify systematic patterns that are unlikely to lead to the final cleaning program, and explore the rest of the space.  
The features guide \sys towards those data cleaning methods/parameter settings that are most promising on previous blocks.
 \sys uses a linear classifier because it can be trained quickly with few examples.   However, we speculate that across a sufficient number of cleaning problems that share common set of data transformations (say, within the same department), we may adapt a deep learning approach to automatically learn the features themselves. 


\iffalse
\stitle{Faster Data Quality Refinement}
\ewu{I emphasize this assuming the experiment exists!!!}
One benefit of learning a pruning model for {\it data transformations} rather than relation instances is that it can potentially be reused or fine-tuned for new, but structurally similar, data cleaning problems.  For instance, when users iteratively refine the data quality measure, the problem is largely the same, and \sys can intialize the search with the previously learned model.  This has the potential to learn  {\it across data cleaning problems} rather than across blocks within a single problem.   \ewu{We evaluate the benefits of re-using models during iterative refinement in Section~\ref{XXX}.}
\fi



 

% For example, some columns might not be dirty and are not worthwile to clean.  In the example above, another observation could be that the source and target strings in the optimal sequence are very close in terms of string similarity (as opposed to arbitrary transformations).  If each of these operations was featurized with a single scalar that is the edit-distance between the two strings, then the classifier could learn a pruning threshold (i.e., not considering find-and-replace operations above that threshold).  

% Consider an alternative to a predefined search heuristic where we clean data in small blocks.  For the initial blocks, we search without a heuristic.  As we continuously perform the search, we train a classifier on these features to reject search branches that are not typically in the final solution.  This allows us to exploit any patterns in the literal parameters that repeatedly occur.






