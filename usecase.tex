\section{An Illustrative Example}


In this case, we are lucky; the joint optimization converges to a reasonable solution given enough time.  However, as we scale to more complex and larger scale problems, avoiding local optima becomes a daunting challenge.  Interactions between methods may not simply be redundancies, and improperly tuned techniques might even reverse the changes made by previous steps.  We arrived at a simple architecture to address this issue by rethinking the parameter search process.  For a given parameter setting, every data cleaning method in the pipeline suggests \emph{candidate} repairs to a dataset.  These repairs are aggregated into a large set of possible transformations to apply Then, a meta-algorithm searches over sequential compositions of the set of generated candidates.  The size of the search space can be controlled by how many parameter settings are considered and how coarse- or fine-grained the repairs are.  This process allows the system to leverage partial repairs from the pipeline when beneficial and naturally avoid problems of redundancy.


More importantly, this generate-then-search process is specialized to the idiosyncrasies of data cleaning.  Data cleaning is fundamentally a form of constraint satisfaction---easy to verify but hard to satisfy.  So, candidate generation can be much more expensive than accuracy evaluation (e.g., for Denial Constraints).  Existing search algorithms implicitly pipeline these two steps; whereas, we decouple these two steps where candidate repairs can be generated in a separate thread and the search algorithm can proceed independently.


In data cleaning, there is a chicken-and-egg problem, where accuracy is defined in terms of recovering the true values of cells in a database, but knowing the true values in advance would defeat the purpose of cleaning.  In practice, we often use a proxy for accuracy such as the satisfaction of integrity constraints, how well it fits a statistical model, or accuracy on gold standard data.  Given one of these proxy accuracy metrics, is black-box hyperparameter tuning sufficient for applications in data cleaning optimization?

% These systems are alternatively described as ``black box'' search algorithms as they make few assumptions about the underlying structure of the parameter space---simply put, generality at the cost of runtime.  In machine learning, the objective of the search process is clear: the prediction error on a validation set gives an estimate of accuracy. 


A parameter generator thread supplies assignments to each of the data cleaning methods in the library and yields candidate repairs as they are generated to a set.  A greedy tree-search algorithm runs in parallel by sequencing candidate repairs and computing the resulting accuracy measure.  \sys can also execute in a divide-and-conquer fashion by generating candidate parameters on different partitions of data.  It also provides utilities for leveraging information across partitions, by learns a prediction model to estimate the expected improvement of different transformation operators (e.g., some library components might be irrelevant).  

