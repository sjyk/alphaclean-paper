\section{Existing Data Cleaning Systems}
Existing data cleaning systems couple two concepts: the language for specifying the data model and the algorithm to enforce this specification.
Each different family of systems is presented as an end-to-end solution for data cleaning.
In \sys, we take a different perspective and decouple these aspects.
We show that most data models can be written as a scoring function that determines the extent to which the current database instance matches the specification.
Then, we have a single general-purpose search algorithm that finds transformations.
This simplifies the design of these systems and makes it more flexible to apply them to domain specific use cases.

\subsection{Constraint-based Systems}
\textbf{SK. citations missing}
The classical model for data cleaning is using integrity constraints. The data scientist describes the relationships between attributes in terms of functional dependencies, conditional functional dependencies, denial constraints, and other constraints.
Then, if any of these constraints are violated, the system will search over updates to the inconsistent database instance to enforce the constraints.
Most search algorithms are based on the ``chase'', which is a fixed-point iteration technique. The chase iteratively enforces a constraint for each tuple in violation (possibly introducing new violations), and iterates until satisfaction.
Under suitable conditions the chase will converge to a fixed-point.

One issue with the constraint-based model is that not all satisfying instances are created equal. For example, we may additionally want to minimize the number of edits to the database instance.
Sometimes this edit cost more complex than simply counting the number of cells that are modified, e.g., information disclosure, minimize disagreement with statistical model.
For these costs, the solution algorithms become substantially more complex and leverage different heuristics.
We propose simplifying the design space by decoupling the search algorithm from the constraint specification.
There is a cost for violating a constraint and there is a cost for editing the database.
The total cost is a sum of the two terms.
We can use general purpose search algorithm to optimize this objective function.
Clearly, the generalized approach will be slower than the special purpose systems, but the added generality is very helpful.
For example, we present an experiment on US Election contribution data with matching dependencies with an ``exotic'' editing cost determined by a pre-trained word2vec model. This performs a form of entity resolution where similarity is semantic rather than based on string similarity.

\subsection{Statistical Model-based Systems}
Another class of systems uses statistical properties to define data errors. Statistical models discriminatively define normal v.s. abnormal records.
For each type of abnormality, the system associates a repair (e.g., deleting the record or setting it to a default value).
These systems are on the opposite side of the tradeoff space to the constraint-based systems.
In a sense, the data model is highly expressive, for example, the model that determines anomalies can be neural network. However, the edits that can be made to the database a very simple rules.

\textbf{SK. citations missing}

However, these systems actually fit into the same design paradigm as the constraint-based systems.
There is a function that scores a relation in terms of the amount of anomalous data (e.g., the number and magnitude of records 3 std. above the mean). There is an allowed language for editing the database (e.g., deletions and default value imputation).
The search algorithm is to find transformations that optimize the objective.

One benefit of casting statistical data cleaning in this formalism is improved handling of mixed datasets.
For example, some datasets might have mixes of numerical and categorical data--both of which are dirty.
We present an experiment using rainfall data from Ethiopia, where \sys is used to clean a dataset with both numerical and syntactic errors.

\subsection{Demonstration-based Systems}
Finally, there are a handful of systems that adopt a programming-by-demonstration (PbD) approach to data cleaning.
A human analyst manually cleans example records and the system searches to find a concise program that mimics this behavior.

\textbf{SK. citations missing. Joes work and Cafferella's work}


Such approaches are clearly linked to \sys. The quality function is a score on each record on how accurately it matches a manually cleaned gold-standard. \sys can additionally couple automated and PbD approaches. We present an example on a survey dataset from Phillipines where manual cleaning is needed for domain specific content but automated cleaning can be used for generic transformations.












