\section{Problem Definition}
First, we overview the basic formalism of \sys and present its relationship to related work.

\subsection{Data Transformations}
We focus on data transformations that concern a single relational table. 
Let $R$ be an instance over a set of attributes $A$, and let $\mathcal{R}$ denote the set of all possible instances over $A$.
Let $r.a$ be the attribute value of $a \in A$ for row $r \in R$.
$T(R): \mathcal{R} \mapsto \mathcal{R}$ is a data transformation that maps an input relation instance $R \in \mathcal{R}$ to a new (possibly cleaner) instance $R' \in \mathcal{R}$ that adheres to the same schema.  For instance, ``replace all \texttt{city} attribute values equal to {\it San Francis} with {\it SF}'' may be one data transformation, while ``delete the $10^{th}$ record'' may be another.   Data transformations can be composed using the binary operator $\circ$:
\[
(T_i \circ T_j)(R) =  T_i(T_j(R))
\]
The composition of one or more data transformations is called a {\it cleaning program} $l$.   

In practice, users will specify {\it transformation templates} $T(R, [\theta_1,\cdots,\theta_k])$, and every assignment to the parameters represents one possible transformation.  
\begin{example}\label{ex1}
The following relation contains two attributes \textsf{city\_name} and \textsf{city\_code}.  Suppose there is a one-to-one relationship between the two attributes. In this case, the relation is inconsistent with respect to the relationship and contains errors highlighted in \red{red}.

  \begin{table}[ht!]
  \centering
  \label{my-label}
  \begin{tabular}{|l|l|l|}
  \hline
  \rowcolor[HTML]{000000} 
  & \white{city\_name}            & \white{city\_code}   \\ \hline
  1 & San Francisco                    & SF                                  \\ \hline
  2& \red{\textbf{New York}}           & NY                                  \\ \hline
  3 & New York City                    & \red{\textbf{NYC}} \\ \hline
  4 & \red{\textbf{San Francisc}}      & SF                                  \\ \hline
  5 & San Jose                         & SJ                                  \\ \hline
  6 & San Mateo                        & SM                                  \\ \hline
  7 & New York City                    & NY                                  \\ \hline
  \end{tabular}
  \end{table}

The following transformation template uses three parameters: \texttt{attr} specifies an attribute, \texttt{srcstr} specifies a source string, and \texttt{targetstr} specifies a target string.   
\[
\textsf{find\_replace}(\text{srcstr}, \text{targetstr}, \text{attr})
\]
Given an input relation, the template finds all \texttt{attr} values equal to \texttt{srcstr} and replaces those cells with \texttt{targetstr}. 
For instance, \texttt{find\_replace(``NYC'', ``NY'', ``city\_code'')} defines a data transformation that fixes the error in the second attribute.
\end{example}

Let $\Sigma$ be a set of distinct data transformations $\{T_1,\cdots,T_N\}$, and
$\Sigma^*$ be the set of all finite compositions of $\Sigma$, i.e., $T_i\circ T_j$.
A formal language $L$ over $\Sigma$ is a subset of $\Sigma^*$.
A program $l$ is valid if it is an element of $L$.

\begin{example}\label{ex2}
  Continuing \Cref{ex1}, $\Sigma$ is defined as all possible parameterizations of \texttt{find\_replace}.  Since many possible possible parameterizations are non-sensical (e.g., the source string does not exist in the relation), we may bound $\Sigma$ to only source and target strings present in each attribute's instance domain~\cite{workthatdoesthis}.  In this case, there are $61$ possible data transformations, and $\Sigma^*$ defines any finite composition of these $61$ transformations.  The language $L$ can be further restricted to compositions of up to $k$ data transformations.  
\end{example}

Finally, let $Q(R): \mathcal{R} \mapsto [0,1]$ be a quality function where $1$ implies that the instance $R$ is clean.
Although this general formulation captures arbitatry forms of evaluation, including crowdsourced quality validation~\cite{tamr,stuff}, common data cleaning algorithms~\cite{} evaluate the cleanliness of in instance by evaluating the relation on a row-by-row or cell-by-cell basis.  For this reason, we define two classes of quality functions: row-separable and cell-separable quality functions.
The former expresses the overall quality based on row-wise quality function $q(r): R \mapsto [0,1]$ where $1$ implies that the record is clean:
\[Q(R) \propto \sum_{r \in R} q(r)\]
\noindent Similarly, a cell-separable quality function can be expressed based on a cell-wise quality function $q(r, a): (R\times A) \mapsto [0,1]$:
\[Q(R) \propto \sum_{r \in R} \sum_{a \in A} q(r,a)\]
\noindent We are now ready to present data cleaning as the following optimization problem:
\begin{problem}[Cleaning Synthesis Problem]
Given a quality function $Q$, a relation $R$, and a language $L$, find valid program $l \in L$ that optimizes the quality function.
\ewu{This used to minimize Q.  I changed to maximize based on above text.  Make sure we are consistent throughout the paper}
\[
\textsf{opt}(Q,R,L) = ~ \max_{l \in L} Q( l(R) ).  
\]
\end{problem}

\begin{example}\label{ex3}
Continuing~\Cref{ex1}, let us assume the following functional dependencies over the example relation: $\textsf{city\_name} \rightarrow \textsf{city\_code}$ and $\textsf{city\_code} \rightarrow \textsf{city\_name}$.
We can efficiently identify inconsistencies by finding the cities that map to $>1$ city code, and vice versa.   Let such city names and codes be denoted $D_{city\_name}$ and $D_{city\_code}$, respectively.
$Q(R)$ is a cell-separable quality function where the cell-wise quality function is defined as $q(r, a) = 1 - (r.a \in D_a)$, such that $r.a$ is $1$ if the attribute value does not violate a functional dependency, and $0$ otherwise.

By searching through all possible programs up to length \ewu{3} in $L$, we can find a cleaning program based on \texttt{find\_replace} that resolves all inconsistencies:
\begin{lstlisting}
    find_replace(New York, New York City, city_name)
    find_replace(San Francisc, San Francisco, city_name)
    find_replace(NYC, NY, city_code)
\end{lstlisting}
\end{example}







\subsection{Model Expressiveness}
We now show how our problem formulation can be used to express a wide range data cleaning problems that have traditionally be tackled in isolation.  These span three major classes of problems: constraint-based cleaning that use integrity constraints to identify and reconcile errors, statistical model-based  cleaning that identify and fix data values that fall outside of an expected statistical model, and programming by demonstration approaches that synthesize cleaning programs based on user-provided example outputs.   The next subsection describes the key technical challenge.

% Since the beginning of data management, several research and commercial systems have been proposed to improve data cleaning efficiency and accuracy (see~\cite{rahm2000data} for a survey).  These have primarily been addressed as three separate classes of data cleaning problems---constraint-based cleaning that use integrity constraints to identify and reconcile errors, statistical model-based  cleaning that identify and fix data values that fall outside of an expected statistical model, and programming by demonstration approaches that synthesize cleaning programs based on user-provided example outputs.  Solutions, measures of cleanliness, and optimizations have been developed for each class independently.  The rest of this subsection describes how each class can be cast as instances of our problem formulation. 

\subsubsection{Constraint-based Systems}
Beginning with Codd's seminal paper that introduced the relational model, the formal notion of data errors is defined with respect to the set of integrity constraints defined over the database.   These can vary from attribute constraints such as type and domain constraints; to multi-attribute constraints such as functional dependencies, conditional functional dependencies, and denial constraints; to general \texttt{CHECK} constraints.   
\ewu{Solutions use variatinos of the CHASE}.
Then, if any of these constraints are violated, the system will search over updates to the inconsistent database instance to enforce the constraints.
Recent example systems include NADEEF~\cite{DBLP:conf/sigmod/DallachiesaEEEIOT13}, LLunatic~\cite{geerts2013llunatic}, Holistic Data Cleaning~\cite{chu2013holistic}, and Big Dansing~\cite{khayyat2015bigdansing}.
\ewu{Clarify that each system is specialized to a different type or combination of integrity constraints.  Why?   The commonality is that, similar to our running example, they seek to maximize a quality function tha measures the number of constraint violations (at the cell~\cite{} or record~\cite{} level).  }
Since these systems return a cleaned database, their language can be modeled with a transformation template \texttt{fix(r,a,v)} that replaces a specific cell for record $r$ and attribute $a$ with a value $v$.  Typically $v$ is restricted to the values in the instance's attribute domain.

While prior work are roughly speaking equivalent in expressiveness, they each have a different solution algorithm.
For example, Dallachiesa et al. uses a SAT solver to enforce the constraints~\cite{DBLP:conf/sigmod/DallachiesaEEEIOT13}, Chu et al. uses a iterative algorithm that walks along a hypergraph~\cite{chu2013holistic}, and Geerts et al. is based on a fixed-point iteration~\cite{geerts2013llunatic}.
Our goal with \sys is to reduce the complexity of such systems, by consolidating to a single general-purpose search algorithm and an API for specifying optimizations specific to particular problem classes.
This means that the approach is more general and can handle novel combinations of data cleaning operations, e.g., denial constraints and numerical outliers.

\subsubsection{Statistical Cleaning}
Another class of systems uses statistical properties to define data errors, which is also called Quantitative Data Cleaning (see survey by Hellerstein~\cite{hellerstein2008quantitative}).
A user can define a statistical model that the data should conform to, e.g., all numerical values must be concentrated with $p$ standard deviations from the mean. 
More generally, one can use a model to score records to see how likely those value combinations are.
When presented with data that explained well by this model, the system tries to replace the value.
Example systems include Eracer~\cite{eracer}, SCARED~\cite{yakout2013don}, and dBOOST~\cite{pit2016outlier}.

Statistical quantitative cleaning is a natural fit for \sys, since the record's deviation from the statistical model can be easily translated into a cell or record-wise quality function.  
\ewu{For instance, XXX paper~\cite{} supports XXX Language that can be modeled as a transformation template YYY.
Similarly, XXX paper~\cite{} adopts a cell-separable quality function where YYY.}

% For example, one can chose to delete or impute with a default value. Each cell is scored by a an anomaly detector which marks a percentage confidence that the element is an anomaly (e.g., by the distance to the nearest neighbor). The quality function is the average score over the entire table.
% The search problem is to find a sequence of transformations that reduces the anomaly detection scores.

Recent work on text-featurization such as word embeddings show how similar model-based cleaning techniques can be applied to textual data.  
\ewu{Elaborate}
\ewu{Functions are models, so lookup tables can be viewed as functional dependencies and as a model over an attribute.  }


\if{0}
Similar techniques can be applied to non-numerical data if an appropriate featurization is used.
One approach is to borrow recent results from Natural Language Processing using Neural Networks to first embed the records in a vector-space and then apply numerical outlier detection techniques. The \textsf{word2vec} model \cite{mikolov2013distributed} is one such approach.
Using large amounts of unannotated plain text, \textsf{word2vec} learns relationships between words automatically with a Neural Network that predicts the occurrence of nearby words.
 Each word is assigned a vector in the vector space such that words that share common contexts (i.e., occur in the same document) in the corpus are located in close proximity to one another in the space.
 This vector space captures semantic relationships between words.
 
Each record is treated as a document and each attribute is treated as word.
 The model is then trained using all of the records in the training dataset.
 Thus, for each attribute value we have a vector.
 To featurize a record, we concatenate these vectors together.
 Therefore, for each record $r$ there is an associated vector $r_v$.
  Like the NLP application, this vector space captures semantic relationships between records.
  We can define the same anomaly detection objective as above over these features.
\fi
  
\subsubsection{Programming-by-Demonstration}
Finally, \sys is very related to systems that apply programming-by-demonstration (PbD) approach to data preparation problems.
This approach was most notably proposed in the Data Wrangler project, where a human provided initial examples of how to transform a set of semi-structured tuples into a structured schema~\cite{wrangler,trifacta}.
The system performed a search to find a sequence of transformations to best reproduce the humans demonstration.
Recently, this basic approach has been extended in the Foofah system~\cite{jin2017foofah}.
\ewu{Talk about Scorpion.  These are basically constraints over the output, and generating programs to satisfy them.}

Such approaches are clearly linked to \sys. The quality function is a score on each record on how accurately it matches a manually cleaned gold-standard. \sys can additionally couple automated and PbD approaches. 
Consider the example table as before (where managers can't earn less than employees), but this time  a human provides an example repair.
The quality function measures the degree to which the table matches the human examples after applying the data transformations.












\subsection{Approach Overview and Key Challenges}

Our problem forumlation is a direct instance of {\it planning}~\cite{planningbook}, where an agent identifies a sequence of actions to achieve a goal.   In our setting, the agent (\sys) explores a state space ($\mathcal{R}$) from an initial state (the input relation) by following transitions (applying $T_i \in \Sigma$) such that the sequence of actions is valid (within $\Sigma^*$) and the quality of the final state ($Q(R_{final})$) is maximized.  

One may be hesitant in adopting our problem formulation because, although it is sufficiently general to model many existing data cleaning problems, such generality often comes at the expense of runtime performance.   Unfortunately, the cleaning synthesis is APX-Hard, meaning there does not exist a polynomial time approximation unless P=NP.  For this reason, {\it the key technical challenge is to show that \sys can solve data cleaning problems with comparable run-time as existing specialized systems, and can be easily extended to support new optimizations.}

\begin{theorem}[Cleaning Synthesis is APX-Hard]
We show a proof sketch that the cleaning synthesis problem is APX-Hard---
Let $R$ be a single-attribute relation of Booleans. Let $L$ be the set of all assignments to a single value.
Given a list of $N$ Boolean clauses over all the boolean variables, let $Q$ assign to each record one minus the fraction of clauses that evaluate to true. This formulation is equivalent to MAX-SAT and solution to the optimization problem. $\QEDB$
\end{theorem}

Despite the problem complexity, recent successes in similar planning problems---ranging from AlphaGo that \ewu{does something}, and \ewu{Another example}---highlight the potential for practical solutions by leveraging the structure of the problem.  To this end, \sys uses a standard forward state-space algorithm that incrementally grows a tree of operators and prunes paths below a threshold of the best path so far. 

Although this naive formulation is unacceptably slow for even small problems, we show that the algorithm is flexible enough to adapt existing data cleaning optimizations as pruning rules, as well as accomodate novel pruning heuristics (\Cref{xxx}).  In addition, it is simple enough exploit the parallelism available on modern multi-core and cloud compute infrastructures (\Cref{xxx}).

\ewu{TODO: Ask SJ what he is parallelizing}

\if{0}
    Every data cleaning problem in \sys is specified by a deterministic finite automaton (DFA). 
    A DFA is a 5-tuple:
    \[\langle S, A, \delta, s_0\rangle,\]
    where $S$ is a set of states that the process can be in, $A$ is a set of inputs that the process can take, $\delta$ is a transition function that takes as input a state and an input and transitions the process to the next state in $S$, and $s_0$ is an initial state of the process.

    The set of relations and the language of transformations defines a DFA:
    \[\langle \mathcal{R}, \Sigma, \delta, R_{dirty}\rangle, \]
    where the states $\mathcal{R}$ is the set of possible instances, the inputs are transformations from $\Sigma$, the transition function updates the instance with the transformation, and the initial state is the dirty instance $R_{dirty}$. Search problems can be defined over the DFA. 
    A quality function $Q$ maps an instance $R$ to a scalar where 1 implies it is clean:
    \[
    Q: \mathcal{R} \mapsto [0,1]
    \]
    A separable quality function is one that can be expressed as an average over cell-wise quality metrics $q(r,a)$ where 1 implies clean:
    \[
    Q(R) \propto \sum_{r \in R} \sum_{a \in A} q(r,a)
    \]

    Note that in contrast to traditional programming-by-example problems~\cite{}, which take as input a well defined input and goal state, the latter is not present in our problem formulation.  It is replaced with a quality function that we seek to maximize.  This is an important distinction because (1) it models the uncertain nature of data cleaning, where the ground truth is typically not available~\cite{}, and (2) necessarily greedy.

    \vspace{0.5em} \noindent \textbf{Remark 1: } For readers familiar with stochastic processes, this DFA is equivalent to a deterministic Markov Decision Process (MDP), where the states are $\mathcal{R}$, the actions $\Sigma$, the transition function updates the instance with the transformation, the initial state is the dirty instance $R_{dirty}$, and the reward function is $Q$. MDPs are usually stochastic and as such the optimal solution in general is not a sequence but a function from states to actions.



\fi







