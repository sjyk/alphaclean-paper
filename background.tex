\section{Problem Definition}
First, we overview the basic formalism of \sys and present its relationship to related work.

\subsection{Data Transformations}
We focus on data transformations that concern a single relational table. 
Let $R$ be a relation over a set of attributes $A$, and let $\mathcal{R}$ denote the set of all possible relations over $A$.
Let $r.a$ be the attribute value of $a \in A$ for row $r \in R$.
$T(R): \mathcal{R} \mapsto \mathcal{R}$ is a data transformation that maps an input relation instance $R \in \mathcal{R}$ to a new (possibly cleaner) instance $R' \in \mathcal{R}$ that adheres to the same schema.  For instance, ``replace all \texttt{city} attribute values equal to {\it San Francisco} with {\it SF}'' may be one data transformation, while ``delete the $10^{th}$ record'' may be another.   Data transformations can be composed using the binary operator $\circ$:
\[
(T_i \circ T_j)(R) =  T_i(T_j(R))
\]
The composition of one or more data transformations is called a {\it cleaning program} $p$.   If $p = p' \circ T$, then let $p'$ be the parent of $p$; the parent of a single data transformation is a NOOP.  

In practice, users will specify {\it transformation templates} $T(R, [\theta_1,\cdots,\theta_k])$, and every assignment to the parameters represents one possible transformation.  
\begin{example}\label{ex1}
The following relation contains two attributes \textsf{city\_name} and \textsf{city\_code}.  Suppose there is a one-to-one relationship between the two attributes. In this case, the relation is inconsistent with respect to the relationship and contains errors highlighted in \red{red}.

  \begin{table}[ht!]
  \centering
  \label{my-label}
  \begin{tabular}{|l|l|l|}
  \hline
  \rowcolor[HTML]{000000} 
  & \white{city\_name}            & \white{city\_code}   \\ \hline
  1 & San Francisco                    & SF                                  \\ \hline
  2& \red{\textbf{New York}}           & NY                                  \\ \hline
  3 & New York City                    & \red{\textbf{NYC}} \\ \hline
  4 & \red{\textbf{San Francisc}}      & SF                                  \\ \hline
  5 & San Jose                         & SJ                                  \\ \hline
  6 & San Mateo                        & SM                                  \\ \hline
  7 & New York City                    & NY                                  \\ \hline
  \end{tabular}
  \end{table}

The following transformation template uses three parameters: \texttt{attr} specifies an attribute, \texttt{srcstr} specifies a source string, and \texttt{targetstr} specifies a target string.   
\[
\textsf{find\_replace}(\text{srcstr}, \text{targetstr}, \text{attr})
\]
Given an input relation, the template finds all \texttt{attr} values equal to \texttt{srcstr} and replaces those cells with \texttt{targetstr}. 
For instance, \texttt{find\_replace(``NYC'', ``NY'', ``city\_code'')} defines a data transformation that fixes the error in the second attribute.
\end{example}

Let $\Sigma$ be a set of distinct data transformations $\{T_1,\cdots,T_N\}$, and
$\Sigma^*$ be the set of all finite compositions of $\Sigma$, i.e., $T_i\circ T_j$.
A formal language $L$ over $\Sigma$ is a subset of $\Sigma^*$.
A program $p$ is valid if it is an element of $L$.

\begin{example}\label{ex2}
  Continuing \Cref{ex1}, $\Sigma$ is defined as all possible parameterizations of \texttt{find\_replace}.  Since many possible possible parameterizations are non-sensical (e.g., the source string does not exist in the relation), we may bound $\Sigma$ to only source and target strings present in each attribute's instance domain (a standard assumption in other work as well~\cite{DBLP:series/synthesis/2012Fan}).  In this case, there are $61$ possible data transformations, and $\Sigma^*$ defines any finite composition of these $61$ transformations.  The language $L$ can be further restricted to compositions of up to $k$ data transformations.  
\end{example}

Finally, let $Q(R): \mathcal{R} \mapsto [0,1]$ be a quality function where $1$ implies that the instance $R$ is clean.
In other words, $Q$ scores each possible result of applying a program $p \in \mathcal{L}$ to an initial dirty table $R_{dirty}$.
There are two special cases that we also consider that give us more information. 
We define two sub-classes of quality functions: row-separable and cell-separable quality functions.
The former expresses the overall quality based on row-wise quality function $q(r): R \mapsto [0,1]$ where $1$ implies that the record is clean:
\[Q(R) \propto \sum_{r \in R} q(r)\]
\noindent Similarly, a cell-separable quality function can be expressed based on a cell-wise quality function $q(r, a): (R\times A) \mapsto [0,1]$:
\[Q(R) \propto \sum_{r \in R} \sum_{a \in A} q(r,a)\]
These special cases are important because they can define hints on what types of transformations are irrelevant.
For example, if we have a cell-separable quality function, then we know we can restrict the language of transformations to only those that modify erroneous cells and exclude those that make no change to them.

\noindent We are now ready to present data cleaning as the following optimization problem:
\begin{problem}[Optimization Problem]
Given a quality function $Q$, a relation $R_{dirty}$, and a language $L$, find valid program $p \in L$ that optimizes the quality function.
\[
\textsf{clean}(Q,R_{dirty},L) = ~ \max_{p \in L} Q( p(R_{dirty}) ).  
\]
\end{problem}
$\textsf{clean}(Q,R_{dirty},L)$ defines a particular instance of the optimization problem, $p^*$ denotes the optimal program, and $p^*(R_{dirty})$ denotes the cleaned table.


\begin{example}\label{ex3}
Continuing~\Cref{ex1}, let us assume the following functional dependencies over the example relation: $\textsf{city\_name} \rightarrow \textsf{city\_code}$ and $\textsf{city\_code} \rightarrow \textsf{city\_name}$.
We can efficiently identify inconsistencies by finding the cities that map to $>1$ city code, and vice versa.   Let such city names and codes be denoted $D_{city\_name}$ and $D_{city\_code}$, respectively.
$Q(R)$ is a cell-separable quality function where the cell-wise quality function is defined as $q(r, a) = 1 - (r.a \in D_a)$, such that $r.a$ is $1$ if the attribute value does not violate a functional dependency, and $0$ otherwise.

By searching through all possible programs up to length 3 in $L$, we can find a cleaning program based on \texttt{find\_replace} that resolves all inconsistencies:
\begin{lstlisting}
    find_replace(New York, New York City, city_name)
    find_replace(San Francisc, San Francisco, city_name)
    find_replace(NYC, NY, city_code)
\end{lstlisting}
\end{example}


\subsection{Model Expressiveness}
We now show how $\textsf{clean}(Q,R_{dirty},L)$ can be used to express a wide range data cleaning problems that have traditionally be tackled with specialized systems.  We present examples from three major classes of problems: constraint-based cleaning that use integrity constraints to identify and reconcile errors, statistical model-based  cleaning that identify and fix data values that fall outside of an expected statistical model, and programming by demonstration approaches that synthesize cleaning programs based on user-provided example outputs.  

\subsubsection{Constraint-based Systems}
Beginning with Codd's seminal paper that introduced the relational model, the formal notion of data errors is defined with respect to the set of integrity constraints defined over the database~\cite{codd1970relational}.   These can vary from attribute constraints such as type and domain constraints; to multi-attribute constraints such as functional dependencies, conditional functional dependencies, and denial constraints; to general \texttt{CHECK} constraints.   
Then, if any of these constraints are violated, the system will search over updates to the inconsistent database instance to enforce the constraints.

Recent example systems include NADEEF~\cite{DBLP:conf/sigmod/DallachiesaEEEIOT13}, LLunatic~\cite{geerts2013llunatic}, Holistic Data Cleaning~\cite{chu2013holistic}, and BigDansing~\cite{khayyat2015bigdansing}.
All of these systems are specialized to a class of integrity constraints called Denial Constraints.
Denial Constraints are a logical language for specifying integrity constraints with universal quantification (e.g., all employees must earn less than his or her manager).
These are specified in the following form:
\[
dc: \forall r_a, r_b \in R \neg \phi(r_a, r_b),
\]
where $\phi(r_a, r_b)$ is a symmetric Boolean function of two tuples.
Denial constraint systems are among the most expressive used in constraint satisfaction.

We can write can write the denial constraint satisfaction problem as a row-separable quality function.
\[Q(r) =
\begin{cases}
0 \text{ if } \exists r_b \in R : \phi(r, r_b) \\
1 \text{ otherwise}
\end{cases}\]
The language $L$ can be modeled with a transformation template \texttt{fix(r,a,v)} that replaces a specific cell for record $r$ and attribute $a$ with a value $v$, where $v$ is restricted to the values in the table's attribute domain.
If we want, we can add an additional penalty on the quality function to discourage edits that deviate greatly from the dirty table:
\[\bar{Q}(r) = Q(r) - \lambda * edit(r_{dirty}, r)\]

Prior work on denial constraints highly specialized solution algorithms.
For example, Dallachiesa et al. uses a SAT solver to enforce the constraints~\cite{DBLP:conf/sigmod/DallachiesaEEEIOT13}, Chu et al. uses a iterative algorithm that walks along a hypergraph~\cite{chu2013holistic}, and Geerts et al. is based on a fixed-point iteration~\cite{geerts2013llunatic}.
Our goal with \sys is to reduce the complexity of such systems, by consolidating to a single general-purpose optimization algorithm.
This means that the approach is more general and can handle novel combinations of data cleaning operations, e.g., denial constraints and numerical outliers.
We will also show in our experiments that the algorithm we consider is easier to scale as it has less assumptions about the data or its dependencies.

\subsubsection{Statistical Cleaning}
Another class of systems uses statistical properties to define data errors, which is also called Quantitative Data Cleaning (see survey by Hellerstein~\cite{hellerstein2008quantitative}).
A user can define a statistical model that the data should conform to, e.g., all numerical values must be concentrated with $t$ standard deviations from the mean. 
More generally, one can use a probabilistic model to score records to see how likely those value combinations are.
When presented with data that explained well by this model, the system tries to replace the value.
Example systems include Eracer~\cite{eracer}, SCARED~\cite{yakout2013don}, and dBOOST~\cite{pit2016outlier}.

Statistical quantitative cleaning is a natural fit for \sys, since the record's deviation from the statistical model can be easily translated into a cell or record-wise quality function.  
Most systems follow a straight-forward pattern, for each record there is a feature vector $f_r$ in $\mathbb{R}^d$.
We fit a probabilistic model $m$ to the data.
For each tuple, we can query the likelihood of the observed feature vector under the model $L(f_r \mid m)$.
Depending on the featurization, we can make this a row-separable or cell-separable quality function:
\[
Q(r) \propto L(f_r \mid m)
\]
Recent work on text-featurization such as word embeddings show how similar model-based cleaning techniques can be applied to textual data.  

For systems like Eracer~\cite{eracer}, SCARED~\cite{yakout2013don}, the transformation language is the same as the one used in denial constraint systems, \texttt{fix(r,a,v)}.
However, in some cases, we may want to only delete cells that seem erroneous, \texttt{delete(r,a,v)}.
In others, we may want to set a default value (e.g., the mean), like \texttt{default(r,a)}.
This gives \sys more flexibility than existing systems to handle numerical data.

  
\subsubsection{Programming-by-Demonstration}
Finally, \sys is very related to systems that apply programming-by-demonstration (PbD) approach to data preparation problems.
This approach was most notably proposed in the Data Wrangler project, where a human provided initial examples of how to transform a set of semi-structured tuples into a structured schema~\cite{wrangler,trifacta}.
The system performed a search to find a sequence of transformations to best reproduce the humans demonstration.
Recently, this basic approach has been extended in the Foofah system~\cite{jin2017foofah}.
This idea is also related to explanation systems such as Scorpion~\cite{DBLP:journals/pvldb/0002M13}, which have humans identify outliers in aggregate queries and describe predicates over the base data that explain those outliers.

Such approaches are clearly linked to \sys. The quality function is a score on each record on how accurately it matches a manually cleaned gold-standard. \sys can additionally couple automated and PbD approaches. 
Consider the example table as before (where managers can't earn less than employees), but this time  a human provides an example repair.
The quality function measures the degree to which the table matches the human examples after applying the data transformations.
In these systems, the language is often highly restricted.
For example, \cite{wrangler, jin2017foofah} use the domain-specific language proposed in~\cite{raman2001potter}.

\subsection{Approach Overview and Challenges}

Our problem formulation is a direct instance of a {\it planning} in AI~\cite{russell1995modern}, where an agent identifies a sequence of actions to achieve a goal.  In our setting, the agent (\sys) explores a state space ($\mathcal{R}$) from an initial state (the input relation) by following transitions (applying $T_i \in \Sigma$) such that the sequence of actions is valid (within $\Sigma^*$) and the quality of the final state ($Q(R_{final})$) is maximized.  

For readers familiar with stochastic processes, this search problem is equivalent to a deterministic Markov Decision Process (MDP), where the states are $\mathcal{R}$, the actions $\Sigma$, the transition function updates the instance with the transformation, the initial state is the dirty instance $R_{dirty}$, and the reward function is $Q$.

One may be hesitant in adopting our problem formulation because, although it is sufficiently general to model many existing data cleaning problems, such generality often comes at the expense of runtime performance.   The planning problem is APX-Hard, meaning there does not exist a polynomial time approximation unless P=NP.  For this reason, {\it the key technical challenge is to show that \sys can solve data cleaning problems with comparable run-time as existing specialized systems, and can be easily extended to support new optimizations.}

\begin{theorem}[Hardness]
\textsf{clean}(Q,R,L) is APX-hard
\end{theorem}
\begin{proof}[Sketch]
Let $R$ be a single-attribute relation of Booleans. Let $L$ be the set of all assignments to a single value.
Given a list of $N$ Boolean clauses over all the boolean variables, let $Q$ assign to each record one minus the fraction of clauses that evaluate to true. This formulation is equivalent to MAX-SAT and solution to the optimization problem. 
\end{proof}

Despite the problem's worst-case complexity, 
recent successes in similar planning problems---ranging from AlphaGo~\cite{silver2016mastering} to automatically playing Atari video games~\cite{mnih2015human} have shown that a prudent combination Machine Learning and distributed search can find practical solutions by leveraging the structure of the problem. 
Not every problem instance is as pathological as the worst case complexity suggests, and there are many reasonable local optima.

\sys uses a standard state-space algorithm that incrementally grows a tree of operators and prunes paths below a threshold of the best path so far. 
This algorithm is combined with an adaptive step that learns a search heuristic that adaptively prunes implausible search branches early.
Additionally, we show that the basic algorithm is flexible enough to adapt existing data cleaning optimizations through pruning rules.  In addition, it is simple enough exploit the parallelism available on modern multi-core and cloud compute infrastructures.








\if{0}
    Every data cleaning problem in \sys is specified by a deterministic finite automaton (DFA). 
    A DFA is a 5-tuple:
    \[\langle S, A, \delta, s_0\rangle,\]
    where $S$ is a set of states that the process can be in, $A$ is a set of inputs that the process can take, $\delta$ is a transition function that takes as input a state and an input and transitions the process to the next state in $S$, and $s_0$ is an initial state of the process.

    The set of relations and the language of transformations defines a DFA:
    \[\langle \mathcal{R}, \Sigma, \delta, R_{dirty}\rangle, \]
    where the states $\mathcal{R}$ is the set of possible instances, the inputs are transformations from $\Sigma$, the transition function updates the instance with the transformation, and the initial state is the dirty instance $R_{dirty}$. Search problems can be defined over the DFA. 
    A quality function $Q$ maps an instance $R$ to a scalar where 1 implies it is clean:
    \[
    Q: \mathcal{R} \mapsto [0,1]
    \]
    A separable quality function is one that can be expressed as an average over cell-wise quality metrics $q(r,a)$ where 1 implies clean:
    \[
    Q(R) \propto \sum_{r \in R} \sum_{a \in A} q(r,a)
    \]

    Note that in contrast to traditional programming-by-example problems~\cite{}, which take as input a well defined input and goal state, the latter is not present in our problem formulation.  It is replaced with a quality function that we seek to maximize.  This is an important distinction because (1) it models the uncertain nature of data cleaning, where the ground truth is typically not available~\cite{}, and (2) necessarily greedy.

    \vspace{0.5em} \noindent \textbf{Remark 1: } 



\fi







