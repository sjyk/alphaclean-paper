\section{Problem Definition}
First, we overview the basic formalism of \sys and present its relationship to related work.

\subsection{Data Transformations}
We focus on data transformations that concern a single relational table. 
Let $R$ be an instance over a set of attributes $A$, and let $\mathcal{R}$ denote the set of all possible instances over $A$.
Let $r.a$ be the attribute value of $a \in A$ for row $r \in R$.
$T(R): \mathcal{R} \mapsto \mathcal{R}$ is a data transformation that maps an input relation instance $R \in \mathcal{R}$ to a new (possibly cleaner) instance $R' \in \mathcal{R}$ that adheres to the same schema.  For instance, ``replace all \texttt{city} attribute values equal to {\it San Francis} with {\it SF}'' may be one data transformation, while ``delete the $10^{th}$ record'' may be another.   Data transformations can be composed using the binary operator $\circ$:
\[
(T_i \circ T_j)(R) =  T_i(T_j(R))
\]
The composition of one or more data transformations is called a {\it cleaning program}.   

In practice, users will specify {\it transformation templates} $T(R, [\theta_1,\cdots,\theta_k])$, and every assignment to the parameters represents one possible transformation.  
\begin{example}\label{ex1}
The following relation contains two attributes \textsf{city\_name} and \textsf{city\_code}.  Suppose there is a one-to-one relationship between the two attributes. In this case, the relation is inconsistent with respect to the relationship and contains errors highlighted in \red{red}.

  \begin{table}[ht!]
  \centering
  \label{my-label}
  \begin{tabular}{|l|l|l|}
  \hline
  \rowcolor[HTML]{000000} 
  & \white{city\_name}            & \white{city\_code}   \\ \hline
  1 & San Francisco                    & SF                                  \\ \hline
  2& \red{\textbf{New York}}           & NY                                  \\ \hline
  3 & New York City                    & \red{\textbf{NYC}} \\ \hline
  4 & \red{\textbf{San Francisc}}      & SF                                  \\ \hline
  5 & San Jose                         & SJ                                  \\ \hline
  6 & San Mateo                        & SM                                  \\ \hline
  7 & New York City                    & NY                                  \\ \hline
  \end{tabular}
  \end{table}

The following transformation template uses three parameters: \texttt{attr} specifies an attribute, \texttt{srcstr} specifies a source string, and \texttt{targetstr} specifies a target string.   
\[
\textsf{find\_replace}(\text{srcstr}, \text{targetstr}, \text{attr})
\]
Given an input relation, the template finds all \texttt{attr} values equal to \texttt{srcstr} and replaces those cells with \texttt{targetstr}. 
For instance, \texttt{find\_replace(``NYC'', ``NY'', ``city\_code'')} defines a data transformation that fixes the error in the second attribute.
\end{example}

Let $\Sigma$ be a set of distinct data transformations $\{T_1,\cdots,T_N\}$, and
$\Sigma^*$ be the set of all finite compositions of $\Sigma$, i.e., $T_i\circ T_j$.
A formal language $L$ over $\Sigma$ is a subset of $\Sigma^*$.

\begin{example}\label{ex2}
  Continuing \Cref{ex1}, $\Sigma$ is defined as all possible parameterizations of \texttt{find\_replace}.  Since many possible possible parameterizations are non-sensical (e.g., the source string does not exist in the relation), we may bound $\Sigma$ to only source and target strings present in each attribute's instance domain~\cite{workthatdoesthis}.  In this case, there are $61$ possible data transformations, and $\Sigma^*$ defines any finite composition of these $61$ transformations.  The language $L$ can be further restricted to compositions of up to $k$ data transformations.  
\end{example}

Finally, let $Q(R): \mathcal{R} \mapsto [0,1]$ be a quality function where $1$ implies that the instance $R$ is clean.
Although this general formulation captures arbitatry forms of evaluation, including crowdsourced quality validation~\cite{tamr,stuff}, common data cleaning algorithms~\cite{} evaluate the cleanliness of in instance by evaluating the relation on a row-by-row or cell-by-cell basis.  For this reason, we define two classes of quality functions: row-separable and cell-separable quality functions.
The former expresses the overall quality based on row-wise quality function $q(r): R \mapsto [0,1]$ where $1$ implies that the record is clean:
\[Q(R) \propto \sum_{r \in R} q(r)\]
\noindent Similarly, a cell-separable quality function can be expressed based on a cell-wise quality function $q(r, a): (R\times A) \mapsto [0,1]$:
\[Q(R) \propto \sum_{r \in R} \sum_{a \in A} q(r,a)\]
\noindent We are now ready to present data cleaning as the following optimization problem:
\begin{problem}[Cleaning Synthesis Problem]
Given a quality function $Q$, a relation $R$, and a language $L$, find a sequence of transformations $l \in L$ (a sequence of inputs to the DFA) that optimizes the quality function.
\[
\textsf{opt}(Q,R,L) = ~ \min_{l \in L} Q( l(R) ).  
\]
\end{problem}

\begin{example}\label{ex3}
Continuing~\Cref{ex1}, let us assume the following functional dependencies over the example relation: $\textsf{city\_name} \rightarrow \textsf{city\_code}$ and $\textsf{city\_code} \rightarrow \textsf{city\_name}$.
We can efficiently identify inconsistencies by finding the cities that map to $>1$ city code, and vice versa.   Let such city names and codes be denoted $D_{city\_name}$ and $D_{city\_code}$, respectively.
$Q(R$ is a cell-separable quality function where the cell-wise quality function is defined as $q(r, a) = 1 - (r.a \in D_a)$, such that $r.a$ is $1$ if the attribute value does not violate a functional dependency, and $0$ otherwise.

By searching through all possible programs up to length \ewu{XXX} in $L$, we can find a cleaning program based on \texttt{find\_replace} that resolves all inconsistencies:
\begin{lstlisting}
    find_replace(New York, New York City, city_name)
    find_replace(San Francisc, San Francisco, city_name)
    find_replace(NYC, NY, city_code)
\end{lstlisting}
\end{example}


\subsection{Approach Overview and Key Challenges}
Talk about planning, incremental search over $L$, hardness.

\vspace{0.5em} \noindent \textbf{Remark 2: } \textsf{opt(Q,R,L)} is a hard problem. In fact, we can show that it is APX-Hard--that is, unless P=NP there does not exist a polynomial time approximation scheme.
Let $R$ be a single-attribute relation of Booleans. Let $L$ be the set of all assignments to a single value.
Given a list of $N$ Boolean clauses over all the boolean variables, let $Q$ assign to each record one minus the fraction of clauses that evaluate to true. This formulation is equivalent to MAX-SAT and solution to the optimization problem.




\if{0}
Every data cleaning problem in \sys is specified by a deterministic finite automaton (DFA). 
A DFA is a 5-tuple:
\[\langle S, A, \delta, s_0\rangle,\]
where $S$ is a set of states that the process can be in, $A$ is a set of inputs that the process can take, $\delta$ is a transition function that takes as input a state and an input and transitions the process to the next state in $S$, and $s_0$ is an initial state of the process.

The set of relations and the language of transformations defines a DFA:
\[\langle \mathcal{R}, \Sigma, \delta, R_{dirty}\rangle, \]
where the states $\mathcal{R}$ is the set of possible instances, the inputs are transformations from $\Sigma$, the transition function updates the instance with the transformation, and the initial state is the dirty instance $R_{dirty}$. Search problems can be defined over the DFA. 
A quality function $Q$ maps an instance $R$ to a scalar where 1 implies it is clean:
\[
Q: \mathcal{R} \mapsto [0,1]
\]
A separable quality function is one that can be expressed as an average over cell-wise quality metrics $q(r,a)$ where 1 implies clean:
\[
Q(R) \propto \sum_{r \in R} \sum_{a \in A} q(r,a)
\]

Note that in contrast to traditional programming-by-example problems~\cite{}, which take as input a well defined input and goal state, the latter is not present in our problem formulation.  It is replaced with a quality function that we seek to maximize.  This is an important distinction because (1) it models the uncertain nature of data cleaning, where the ground truth is typically not available~\cite{}, and (2) necessarily greedy.

\vspace{0.5em} \noindent \textbf{Remark 1: } For readers familiar with stochastic processes, this DFA is equivalent to a deterministic Markov Decision Process (MDP), where the states are $\mathcal{R}$, the actions $\Sigma$, the transition function updates the instance with the transformation, the initial state is the dirty instance $R_{dirty}$, and the reward function is $Q$. MDPs are usually stochastic and as such the optimal solution in general is not a sequence but a function from states to actions.



\fi

\section{Taxonomy of Related Systems}
Since the beginning of data management, several research and commercial systems have been proposed to improve data cleaning efficiency and accuracy (see~\cite{rahm2000data} for a survey).
Next, we describe how different data cleaning paradigms can be cast as a sequential search problem.

\subsection{Constraint-based Systems}
The classical model for data cleaning is using integrity constraints. 
The data scientist describes the relationships between attributes in terms of functional dependencies, conditional functional dependencies, denial constraints, and other types of logical constraint constraints.
Then, if any of these constraints are violated, the system will search over updates to the inconsistent database instance to enforce the constraints.
Recent example systems include NADEEF~\cite{DBLP:conf/sigmod/DallachiesaEEEIOT13}, LLunatic~\cite{geerts2013llunatic}, Holistic Data Cleaning~\cite{chu2013holistic}, and Big Dansing~\cite{khayyat2015bigdansing}.
As in the running example, we can construct a quality function that marks every cell that is in violation with a 0 and every cell that is not with a 1.
The allowed language would be replacing any cell with another value in the attribute domain.

While prior work are roughly speaking equivalent in expressiveness, they each have a different solution algorithm.
For example, Dallachiesa et al. uses a SAT solver to enforce the constraints~\cite{DBLP:conf/sigmod/DallachiesaEEEIOT13}, Chu et al. uses a iterative algorithm that walks along a hypergraph~\cite{chu2013holistic}, and Geerts et al. is based on a fixed-point iteration~\cite{geerts2013llunatic}.
Our goal with \sys is to reduce the complexity of such systems, by consolidating to a single general-purpose search algorithm and an API for specifying optimizations specific to particular problem classes.
This means that the approach is more general and can handle novel combinations of data cleaning operations, e.g., denial constraints and numerical outliers.

\subsection{Model-based Systems}
Another class of systems uses statistical properties to define data errors, which is also called Quantitative Data Cleaning (see survey by Hellerstein~\cite{hellerstein2008quantitative}).
A user can define a statistical model that the data should conform to, e.g., all numerical values must be concentrated with $p$ standard deviations from the mean. 
More generally, one can use a model to score records to see how likely those value combinations are.
When presented with data that explained well by this model, the system tries to replace the value.
Example systems include Eracer~\cite{eracer}, SCARED~\cite{yakout2013don}, and dBOOST~\cite{pit2016outlier}.

Such scoring functions naturally fit into the model of \sys.
A statistical model can define the quality function and the user can chose preferred behavior for abnormal records.
For example, one can chose to delete or impute with a default value. Each cell is scored by a an anomaly detector which marks a percentage confidence that the element is an anomaly (e.g., by the distance to the nearest neighbor). The quality function is the average score over the entire table.
The search problem is to find a sequence of transformations that reduces the anomaly detection scores.

Similar techniques can be applied to non-numerical data if an appropriate featurization is used.
One approach is to borrow recent results from Natural Language Processing using Neural Networks to first embed the records in a vector-space and then apply numerical outlier detection techniques. The \textsf{word2vec} model \cite{mikolov2013distributed} is one such approach.
Using large amounts of unannotated plain text, \textsf{word2vec} learns relationships between words automatically with a Neural Network that predicts the occurrence of nearby words.
 Each word is assigned a vector in the vector space such that words that share common contexts (i.e., occur in the same document) in the corpus are located in close proximity to one another in the space.
 This vector space captures semantic relationships between words.
 
Each record is treated as a document and each attribute is treated as word.
 The model is then trained using all of the records in the training dataset.
 Thus, for each attribute value we have a vector.
 To featurize a record, we concatenate these vectors together.
 Therefore, for each record $r$ there is an associated vector $r_v$.
  Like the NLP application, this vector space captures semantic relationships between records.
  We can define the same anomaly detection objective as above over these features.
  
\subsection{Programming-by-Demonstration}
Finally, \sys is very related to systems that apply programming-by-demonstration (PbD) approach to data preparation problems.
This approach was most notably proposed in the Data Wrangler project, where a human provided initial examples of how to transform a set of semi-structured tuples into a structured schema~\cite{wrangler,trifacta}.
The system performed a search to find a sequence of transformations to best reproduce the humans demonstration.
Recently, this basic approach has been extended in the Foofah system~\cite{jin2017foofah}.

Such approaches are clearly linked to \sys. The quality function is a score on each record on how accurately it matches a manually cleaned gold-standard. \sys can additionally couple automated and PbD approaches. 
Consider the example table as before (where managers can't earn less than employees), but this time  a human provides an example repair.
The quality function measures the degree to which the table matches the human examples after applying the data transformations.








