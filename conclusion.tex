\section{Conclusion and Future Work}
The research community has developed increasingly sophisticated data cleaning methods~\cite{dc, rekatsinas2017holoclean, DBLP:journals/pvldb/KrishnanWWFG16, DBLP:conf/sigmod/ChuIKW16, mudgal2018deep, doan2018toward}.
The burden on the analyst is gradually shifting away from the design of hand-written data cleaning scripts, to building and tuning complex pipelines of automated data cleaning libraries.
The main insight of this paper is that tuning pipelines of data cleaning operations is very different than tuning pipelines for machine learning.

Rather than treating each pipeline component as a black-box transformation of an entire table, \sys extracts the constituent replacements. Given a library of data cleaning methods, each suggests potential replacements policies which are aggregated into a central set. This defines a well-posed plan-space, namely, the set of all compositions of candidate functions. This plan space captures method reordering, method exclusion, and applying a method to a subset of records.
  
Although our results suggest that borrowing from recent advances in planning and optimization is a fruitful direction, the results are counter-intuitive and raise a number of questions about future opportunities in data cleaning.  Does \sys achieve its results benchmarks used in data cleaning are too simple and amenable to greedy brute-force search?  
We hope to really characterize and understand these tradeoffs in the future. We are excited to extend \sys towards a more flexible and usable data cleaning system.  In particular, data cleaning is inherently a visual and interactive process, and we plan to integrate \sys with a data visualization interface.   Users can visually manipulate and examine their dataset and the system can translate interactive manipulations into quality functions.  This will also require work to characterize failure modes and provide high level tools to debug such cases.  We are also hopeful that the compact codebase ($<$200LOC for the core search and learning algorithms) can enable more rapid development of specialized data cleaning systems for novel domains and error conditions.  



% The prevailing wisdom in the design of data cleaning algorithms is to exploit the details of specific problem rather than considering the most general cases, and our experiments suggest that this a general framework like \sys can achieve parity in terms of accuracy.
% While the serial implementation of \sys can be much slower than the competitor specialized frameworks, \sys can be efficiently distributed to significantly reduce the gap.
% 
% These results should be considered a proof-of-concept that such a data cleaning \sys can be built around the recent results in AI. 
% However, to us, these results are still counter-intuitive, and raise a number of speculative questions for the future: (1) are specialized systems overly engineered for worst-case guarantees and perhaps real-datasets are not that pathalogical, (2) maybe the benchmarks that we consider in data cleaning are too easy to brute-force, (3) what are the failure modes and corner cases of \sys in real data.
% We hope to consider these problems in future work, as well as extending the system to novel settings.
% In particular, we are interested in \sys as a middleware layer for data visualization.
% A user can manipulate data in a visual UI and these manipulations can be translated into a quality function.
% 
